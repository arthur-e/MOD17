<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>mod17.calibration API documentation</title>
<meta name="description" content="Calibration of MOD17 against a representative, global eddy covariance (EC)
flux tower network. The model calibration is based on Markov-Chain Monte
…" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mod17.calibration</code></h1>
</header>
<section id="section-intro">
<p>Calibration of MOD17 against a representative, global eddy covariance (EC)
flux tower network. The model calibration is based on Markov-Chain Monte
Carlo (MCMC). Example use:</p>
<pre><code>python calibration.py tune-gpp --pft=1
</code></pre>
<p>The general calibration protocol used here involves:</p>
<ol>
<li>Check how well the chain(s) are mixing by running short:
<code>python calibration.py tune-gpp 1 --draws=2000</code></li>
<li>If any chain is "sticky," run a short chain while tuning the jump scale:
<code>python calibration.py tune-gpp 1 --tune=scaling --draws=2000</code></li>
<li>Using the trace plot from Step (2) as a reference, experiment with
different jump scales to try and achieve the same (optimal) mixing when
tuning on <code>lambda</code> (default) instead, e.g.:
<code>python calibration.py tune-gpp 1 --scaling=1e-2 --draws=2000</code></li>
<li>When the right jump scale is found, run a chain at the desired length.</li>
</ol>
<p>Once a good mixture is obtained, it is necessary to prune the samples to
eliminate autocorrelation, e.g., in Python:</p>
<pre><code>sampler = MOD17StochasticSampler(...)
sampler.plot_autocorr(burn = 1000, thin = 10)
trace = sampler.get_trace(burn = 1000, thin = 10)
</code></pre>
<p>A thinned posterior can be exported from the command line:</p>
<pre><code class="language-py">$ python calibration.py export-bplut output.csv --burn=1000 --thin=10
</code></pre>
<h2 id="references">References</h2>
<p>Madani, N., Kimball, J. S., &amp; Running, S. W. (2017).
"Improving global gross primary productivity estimates by computing
optimum light use efficiencies using flux tower data."
Journal of Geophysical Research: Biogeosciences, 122(11), 2939–2951.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
Calibration of MOD17 against a representative, global eddy covariance (EC)
flux tower network. The model calibration is based on Markov-Chain Monte
Carlo (MCMC). Example use:

    python calibration.py tune-gpp --pft=1

The general calibration protocol used here involves:

1. Check how well the chain(s) are mixing by running short:
`python calibration.py tune-gpp 1 --draws=2000`
2. If any chain is &#34;sticky,&#34; run a short chain while tuning the jump scale:
`python calibration.py tune-gpp 1 --tune=scaling --draws=2000`
3. Using the trace plot from Step (2) as a reference, experiment with
different jump scales to try and achieve the same (optimal) mixing when
tuning on `lambda` (default) instead, e.g.:
`python calibration.py tune-gpp 1 --scaling=1e-2 --draws=2000`
4. When the right jump scale is found, run a chain at the desired length.

Once a good mixture is obtained, it is necessary to prune the samples to
eliminate autocorrelation, e.g., in Python:

    sampler = MOD17StochasticSampler(...)
    sampler.plot_autocorr(burn = 1000, thin = 10)
    trace = sampler.get_trace(burn = 1000, thin = 10)

A thinned posterior can be exported from the command line:

```py
$ python calibration.py export-bplut output.csv --burn=1000 --thin=10
```

References:

    Madani, N., Kimball, J. S., &amp; Running, S. W. (2017).
      &#34;Improving global gross primary productivity estimates by computing
      optimum light use efficiencies using flux tower data.&#34;
      Journal of Geophysical Research: Biogeosciences, 122(11), 2939–2951.
&#39;&#39;&#39;

import datetime
import json
import os
import warnings
import numpy as np
import h5py
import arviz as az
import pymc as pm
import aesara.tensor as at
import mod17
from functools import partial
from multiprocessing import get_context, set_start_method
from numbers import Number
from typing import Callable, Sequence
from pathlib import Path
from matplotlib import pyplot
from scipy import signal
from mod17 import MOD17, PFT_VALID
from mod17.utils import pft_dominant, restore_bplut, write_bplut, rmsd

MOD17_DIR = os.path.dirname(mod17.__file__)
# This matplotlib setting prevents labels from overplotting
pyplot.rcParams[&#39;figure.constrained_layout.use&#39;] = True


class BlackBoxLikelihood(at.Op):
    &#39;&#39;&#39;
    A custom Theano operator that calculates the &#34;likelihood&#34; of model
    parameters; it takes a vector of values (the parameters that define our
    model) and returns a single &#34;scalar&#34; value (the log-likelihood).

    Parameters
    ----------
    model : Callable
        An arbitrary &#34;black box&#34; function that takes two arguments: the
        model parameters (&#34;params&#34;) and the forcing data (&#34;x&#34;)
    observed : numpy.ndarray
        The &#34;observed&#34; data that our log-likelihood function takes in
    x : numpy.ndarray or None
        The forcing data (input drivers) that our model requires, or None
        if no driver data are required
    weights : Sequence or None
        Optional sequence of weights applied to the model residuals (as in
        weighted least squares)
    objective : str
        Name of the objective (or &#34;loss&#34;) function to use, one of
        (&#39;rmsd&#39;, &#39;gaussian&#39;, &#39;kge&#39;); defaults to &#34;rmsd&#34;
    &#39;&#39;&#39;
    itypes = [at.dvector] # Expects a vector of parameter values when called
    otypes = [at.dscalar] # Outputs a single scalar value (the log likelihood)

    def __init__(
            self, model: Callable, observed: Sequence, x: Sequence = None,
            weights: Sequence = None, objective: str = &#39;rmsd&#39;):
        &#39;&#39;&#39;
        Initialise the Op with various things that our log-likelihood function
        requires. The observed data (&#34;observed&#34;) and drivers (&#34;x&#34;) must be
        stored on the instance so the Theano Op can work seamlessly.
        &#39;&#39;&#39;
        self.model = model
        self.observed = observed
        self.x = x
        self.weights = weights
        if objective in (&#39;rmsd&#39;, &#39;rmse&#39;):
            self._loglik = self.loglik
        elif objective == &#39;gaussian&#39;:
            self._loglik = self.loglik_gaussian
        elif objective == &#39;kge&#39;:
            self._loglik = self.loglik_kge
        else:
            raise ValueError(&#39;Unknown &#34;objective&#34; function specified&#39;)

    def loglik(
            self, params: Sequence, observed: Sequence,
            x: Sequence = None) -&gt; Number:
        &#39;&#39;&#39;
        Pseudo-log likelihood, based on the root-mean squared deviation
        (RMSD). The sign of the RMSD is forced to be negative so as to allow
        for maximization of this objective function.

        Parameters
        ----------
        params : Sequence
            One or more model parameters
        observed : Sequence
            The observed values
        x : Sequence or None
            Input driver data

        Returns
        -------
        Number
            The (negative) root-mean squared deviation (RMSD) between the
            predicted and observed values
        &#39;&#39;&#39;
        predicted = self.model(params, *x)
        if self.weights is not None:
            return -np.sqrt(
                np.nanmean(((predicted - observed) * self.weights) ** 2))
        return -np.sqrt(np.nanmean(((predicted - observed)) ** 2))

    def loglik_gaussian(
            self, params: Sequence, observed: Sequence,
            x: Sequence = None) -&gt; Number:
        &#39;&#39;&#39;
        Gaussian log-likelihood, assuming independent, identically distributed
        observations.

        Parameters
        ----------
        params : Sequence
            One or more model parameters
        observed : Sequence
            The observed values
        x : Sequence or None
            Input driver data

        Returns
        -------
        Number
            The (negative) log-likelihood
        &#39;&#39;&#39;
        predicted = self.model(params, *x)
        sigma = params[-1]
        # Gaussian log-likelihood;
        # -\frac{N}{2}\,\mathrm{log}(2\pi\hat{\sigma}^2)
        #   - \frac{1}{2\hat{\sigma}^2} \sum (\hat{y} - y)^2
        return -0.5 * np.log(2 * np.pi * sigma**2) - (0.5 / sigma**2) *\
            np.nansum((predicted - observed)**2)

    def loglik_kge(
            self, params: Sequence, observed: Sequence,
            x: Sequence = None) -&gt; Number:
        r&#39;&#39;&#39;
        Kling-Gupta efficiency.

        $$
        KGE = 1 - \sqrt{(r - 1)^2 + (\alpha - 1)^2 + (\beta - 1)^2}
        $$

        Parameters
        ----------
        params : Sequence
            One or more model parameters
        observed : Sequence
            The observed values
        x : Sequence or None
            Input driver data

        Returns
        -------
        Number
            The Kling-Gupta efficiency
        &#39;&#39;&#39;
        predicted = self.model(params, *x)
        r = np.corrcoef(predicted, observed)[0, 1]
        alpha = np.std(predicted) / np.std(observed)
        beta = np.sum(predicted) / np.sum(observed)
        return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

    def perform(self, node, inputs, outputs):
        &#39;&#39;&#39;
        The method that is used when calling the Op.

        Parameters
        ----------
        node
        inputs : Sequence
        outputs : Sequence
        &#39;&#39;&#39;
        (params,) = inputs
        logl = self._loglik(params, self.observed, self.x)
        outputs[0][0] = np.array(logl) # Output the log-likelihood


class AbstractSampler(object):
    &#39;&#39;&#39;
    Generic algorithm for fitting a model to data based on observed values
    similar to what we can produce with our model. Not intended to be called
    directly.
    &#39;&#39;&#39;

    def get_posterior(self, thin: int = 1) -&gt; np.ndarray:
        &#39;&#39;&#39;
        Returns a stacked posterior array, with optional thinning, combining
        all the chains together.

        Parameters
        ----------
        thin : int

        Returns
        -------
        numpy.ndarray
        &#39;&#39;&#39;
        trace = az.from_netcdf(self.backend)
        return np.stack([ # i.e., get every ith element, each chain
            trace[&#39;posterior&#39;][p].values[:,::thin].ravel()
            for p in self.required_parameters[self.name]
        ], axis = -1)

    def get_trace(
            self, thin: int = None, burn: int = None
        ) -&gt; az.data.inference_data.InferenceData:
        &#39;&#39;&#39;
        Extracts the trace from the backend data store.

        Parameters
        ----------
        thin : int
            Thinning rate
        burn : int
            The burn-in (i.e., first N samples to discard)
        &#39;&#39;&#39;
        trace = az.from_netcdf(self.backend)
        if thin is None and burn is None:
            return trace
        return trace.sel(draw = slice(burn, None, thin))

    def plot_autocorr(self, thin: int = None, burn: int = None, **kwargs):
        &#39;&#39;&#39;
        Auto-correlation plot for an MCMC sample.

        Parameters
        ----------
        thin : int
            Thinning rate
        burn : int
            The burn-in (i.e., first N samples to discard)
        **kwargs
            Additional keyword arguments to `arviz.plot_autocorr()`.
        &#39;&#39;&#39;
        assert os.path.exists(self.backend),\
            &#39;Could not find file backend!&#39;
        trace = az.from_netcdf(self.backend)
        kwargs.setdefault(&#39;combined&#39;, True)
        if thin is None:
            az.plot_autocorr(trace, **kwargs)
        else:
            burn = 0 if burn is None else burn
            az.plot_autocorr(
                trace.sel(draw = slice(burn, None, thin))[&#39;posterior&#39;],
                **kwargs)
        pyplot.show()

    def plot_forest(self, **kwargs):
        &#39;&#39;&#39;
        Forest plot for an MCMC sample.

        In particular:

        - `hdi_prob`: A float indicating the highest density interval (HDF) to
            plot
        &#39;&#39;&#39;
        assert os.path.exists(self.backend),\
            &#39;Could not find file backend!&#39;
        trace = az.from_netcdf(self.backend)
        az.plot_forest(trace, **kwargs)
        pyplot.show()

    def plot_pair(self, **kwargs):
        &#39;&#39;&#39;
        Paired variables plot for an MCMC sample.

        Parameters
        ----------
        **kwargs
            Additional keyword arguments to `arviz.plot_pair()`.
        &#39;&#39;&#39;
        assert os.path.exists(self.backend),\
            &#39;Could not find file backend!&#39;
        trace = az.from_netcdf(self.backend)
        az.plot_pair(trace, **kwargs)
        pyplot.show()

    def plot_partial_score(
            self, observed: Sequence, drivers: Sequence, fit: dict = None):
        &#39;&#39;&#39;
        Plots the &#34;partial effect&#34; of a single parameter: the score of the
        model at that parameter&#39;s current value against a sweep of possible
        parameter values. All other parameters are held fixed at the best-fit
        values.

        Parameters
        ----------
        observed : Sequence
            Sequence of observed values that will be used to calibrate the model;
            i.e., model is scored by how close its predicted values are to the
            observed values
        drivers : list or tuple
            Sequence of driver datasets to be supplied, in order, to the
            model&#39;s run function
        fit : dict or None
            The best-fit parameter values used for those parameters that are
            fixed
        &#39;&#39;&#39;
        trace = az.from_netcdf(self.backend)
        if fit is None:
            # Mean of posterior are &#34;best fit&#34; values
            fit = trace[&#39;posterior&#39;].mean()
        fit_params = list(filter(
            lambda p: p in fit, self.required_parameters[self.name]))
        # The NPP model depends on constants not included in the fit
        constants = []
        if self.name == &#39;NPP&#39;:
            constants = [
                self.params[p]
                for p in [&#39;LUE_max&#39;, &#39;tmin0&#39;, &#39;tmin1&#39;, &#39;vpd0&#39;, &#39;vpd1&#39;]
            ]
        n = len(fit_params)
        nrow = n
        ncol = 1
        if n &gt; 4:
            nrow = 2
            ncol = n - (n // 2)
        fig, axes = pyplot.subplots(
            nrow, ncol, figsize = (n * 2, n), sharey = True)
        i = 0
        for j in range(nrow):
            for k in range(ncol):
                if i &gt;= n:
                    break
                free_param = fit_params[i]
                fixed = np.stack([
                    fit[p].values for p in fit_params
                ])[np.newaxis,:].repeat(30, axis = 0)
                sweep = np.linspace(
                    trace[&#39;posterior&#39;][free_param].min(),
                    trace[&#39;posterior&#39;][free_param].max(), num = 30)
                fixed[:,i] = sweep
                # Need to concatenate GPP parameters at begining of fixed
                scores = -np.array(self.score_posterior(
                    observed, drivers, [
                        [*constants, *f] for f in fixed.tolist()
                    ]))
                axes[j,k].plot(sweep, scores, &#39;k-&#39;)
                axes[j,k].axvline(
                    fit[free_param], color = &#39;red&#39;, linestyle = &#39;dashed&#39;,
                    label = &#39;Posterior Mean&#39;)
                axes[j,k].set_xlabel(free_param)
                axes[j,k].set_title(free_param)
                axes[j,k].legend()
                i += 1
        # Delete the last empty subplot
        if n % 2 != 0:
            fig.delaxes(axes.flatten()[-1])
        axes[0, 0].set_ylabel(&#39;Score&#39;)
        pyplot.tight_layout()
        pyplot.show()

    def plot_posterior(self, **kwargs):
        &#39;&#39;&#39;
        Plots the posterior distribution for an MCMC sample.

        Parameters
        ----------
        **kwargs
            Additional keyword arguments to `arviz.plot_posterior()`.
        &#39;&#39;&#39;
        assert os.path.exists(self.backend),\
            &#39;Could not find file backend!&#39;
        trace = az.from_netcdf(self.backend)
        az.plot_posterior(trace, **kwargs)
        pyplot.show()

    def score_posterior(
            self, observed: Sequence, drivers: Sequence, posterior: Sequence,
            method: str = &#39;rmsd&#39;) -&gt; Number:
        &#39;&#39;&#39;
        Returns a goodness-of-fit score based on the existing calibration.

        Parameters
        ----------
        observed : Sequence
            Sequence of observed values that will be used to calibrate the model;
            i.e., model is scored by how close its predicted values are to the
            observed values
        drivers : list or tuple
            Sequence of driver datasets to be supplied, in order, to the
            model&#39;s run function
        posterior : list or tuple
            Sequence of posterior parameter sets (i.e., nested sequence); each
            nested sequence will be scored
        method : str
            The method for generating a goodness-of-git score
            (Default: &#34;rmsd&#34;)

        Returns
        -------
        float
        &#39;&#39;&#39;
        if method != &#39;rmsd&#39;:
            raise NotImplementedError(&#39;&#34;method&#34; must be one of: &#34;rmsd&#34;&#39;)
        score_func = partial(
            rmsd, func = self.model, observed = observed, drivers = drivers)
        with get_context(&#39;spawn&#39;).Pool() as pool:
            scores = pool.map(score_func, posterior)
        return scores


class StochasticSampler(AbstractSampler):
    &#39;&#39;&#39;
    A Markov Chain-Monte Carlo (MCMC) sampler for an arbitrary model. The
    specific sampler used is the Differential Evolution (DE) MCMC algorithm
    described by Ter Braak (2008), though the implementation is specific to
    the PyMC3 library.

    NOTE: The `model` (a function) should be named &#34;_name&#34; where &#34;name&#34; is
    some uniquely identifiable model name. This helps `StochasticSampler.run()`
    to find the correct compiler for the model. The compiler function should
    be named `compiled_name_model()` (where &#34;name&#34; is the model name) and be
    defined on a subclass of `StochasticSampler`.

    Parameters
    ----------
    config : dict
        Dictionary of configuration parameters
    model : Callable
        The function to call (with driver data and parameters); this function
        should take driver data as positional arguments and the model
        parameters as a `*Sequence`; it should require no external state.
    observed : Sequence
        Sequence of observed values that will be used to calibrate the model;
        i.e., model is scored by how close its predicted values are to the
        observed values
    params_dict : dict or None
        Dictionary of model parameters, to be used as initial values and as
        the basis for constructing a new dictionary of optimized parameters
    backend : str or None
        Path to a NetCDF4 file backend (Default: None)
    weights : Sequence or None
        Optional sequence of weights applied to the model residuals (as in
        weighted least squares)
    &#39;&#39;&#39;
    def __init__(
            self, config: dict, model: Callable, params_dict: dict = None,
            backend: str = None, weights: Sequence = None,
            model_name: str = None):
        self.backend = backend
        # Convert the BOUNDS into nested dicts for easy use
        self.bounds = dict([
            (key, dict([(&#39;lower&#39;, b[0]), (&#39;upper&#39;, b[1])]))
            for key, b in config[&#39;optimization&#39;][&#39;bounds&#39;].items()
        ])
        self.config = config
        self.model = model
        if hasattr(model, &#39;__name__&#39;):
            self.name = model.__name__.strip(&#39;_&#39;).upper() # &#34;_gpp&#34; = &#34;GPP&#34;
        else:
            self.name = model_name
        self.params = params_dict
        # Set the model&#39;s prior distribution assumptions
        self.prior = dict()
        for key in self.required_parameters[self.name]:
            # NOTE: This is the default only for LUE_max; other parameters,
            #   with Uniform distributions, don&#39;t use anything here
            self.prior.setdefault(key, {
                &#39;mu&#39;: params_dict[key],
                &#39;sigma&#39;: 2e-4
            })
        self.weights = weights
        assert os.path.exists(os.path.dirname(backend))

    def run(
            self, observed: Sequence, drivers: Sequence,
            draws = 1000, chains = 3, tune = &#39;lambda&#39;,
            scaling: float = 1e-3, prior: dict = dict(),
            check_shape: bool = False, save_fig: bool = False,
            var_names: Sequence = None) -&gt; None:
        &#39;&#39;&#39;
        Fits the model using DE-MCMCz approach. `tune=&#34;lambda&#34;` (default) is
        recommended; lambda is related to the scale of the jumps learned from
        other chains, but epsilon (&#34;scaling&#34;) controls the scale directly.
        Using a larger value for `scaling` (Default: 1e-3) will produce larger
        jumps and may directly address &#34;sticky&#34; chains.

        Parameters
        ----------
        observed : Sequence
            The observed data the model will be calibrated against
        drivers : list or tuple
            Sequence of driver datasets to be supplied, in order, to the
            model&#39;s run function
        draws : int
            Number of samples to draw (on each chain); defaults to 1000
        chains : int
            Number of chains; defaults to 3
        tune : str or None
            Which hyperparameter to tune: Defaults to &#39;lambda&#39;, but can also
            be &#39;scaling&#39; or None.
        scaling : float
            Initial scale factor for epsilon (Default: 1e-3)
        prior : dict
        check_shape : bool
            True to require that input driver datasets have the same shape as
            the observed values (Default: False)
        save_fig : bool
            True to save figures to files instead of showing them
            (Default: False)
        var_names : Sequence
            One or more variable names to show in the plot
        &#39;&#39;&#39;
        assert not check_shape or drivers[0].shape == observed.shape,\
            &#39;Driver data should have the same shape as the &#34;observed&#34; data&#39;
        assert len(drivers) == len(self.required_drivers[self.name]),\
            &#39;Did not receive expected number of driver datasets!&#39;
        assert tune in (&#39;lambda&#39;, &#39;scaling&#39;) or tune is None
        # Update prior assumptions
        self.prior.update(prior)
        # Generate an initial goodness-of-fit score
        predicted = self.model([
            self.params[p] for p in self.required_parameters[self.name]
        ], *drivers)
        if self.weights is not None:
            score = np.sqrt(
                np.nanmean(((predicted - observed) * self.weights) ** 2))
        else:
            score = np.sqrt(np.nanmean(((predicted - observed)) ** 2))
        print(&#39;-- RMSD at the initial point: %.3f&#39; % score)
        print(&#39;Compiling model...&#39;)
        try:
            compiler = getattr(self, &#39;compile_%s_model&#39; % self.name.lower())
        except AttributeError:
            raise AttributeError(&#39;&#39;&#39;Could not find a compiler for model named
            &#34;%s&#34;; make sure that a function &#34;compile_%s_model()&#34; is defined on
             this class&#39;&#39;&#39; % (model_name, model_name))
        with compiler(observed, drivers) as model:
            with warnings.catch_warnings():
                warnings.simplefilter(&#39;ignore&#39;)
                step_func = pm.DEMetropolisZ(tune = tune, scaling = scaling)
                trace = pm.sample(
                    draws = draws, step = step_func, cores = chains,
                    chains = chains, idata_kwargs = {&#39;log_likelihood&#39;: True})
            if self.backend is not None:
                print(&#39;Writing results to file...&#39;)
                trace.to_netcdf(self.backend)
            if var_names is None:
                az.plot_trace(trace, var_names = [&#39;~log_likelihood&#39;])
            else:
                az.plot_trace(trace, var_names = var_names)
            if save_fig:
                pyplot.savefig(&#39;.&#39;.join(self.backend.split(&#39;.&#39;)[:-1]) + &#39;.png&#39;)
            else:
                pyplot.show()


class MOD17StochasticSampler(StochasticSampler):
    &#39;&#39;&#39;
    A Markov Chain-Monte Carlo (MCMC) sampler for MOD17. The specific sampler
    used is the Differential Evolution (DE) MCMC algorithm described by
    Ter Braak (2008), though the implementation is specific to the PyMC3
    library.

    Considerations:

    1. Tower GPP is censored when values are &lt; 0 or when APAR is
        &lt; 0.1 MJ m-2 d-1.

    Parameters
    ----------
    config : dict
        Dictionary of configuration parameters
    model : Callable
        The function to call (with driver data and parameters); this function
        should take driver data as positional arguments and the model
        parameters as a `*Sequence`; it should require no external state.
    observed : Sequence
        Sequence of observed values that will be used to calibrate the model;
        i.e., model is scored by how close its predicted values are to the
        observed values
    params_dict : dict or None
        Dictionary of model parameters, to be used as initial values and as
        the basis for constructing a new dictionary of optimized parameters
    backend : str or None
        Path to a NetCDF4 file backend (Default: None)
    weights : Sequence or None
        Optional sequence of weights applied to the model residuals (as in
        weighted least squares)
    &#39;&#39;&#39;
    # NOTE: This is different than for mod17.MOD17 because we haven&#39;t yet
    #   figured out how the respiration terms are calculated
    required_parameters = {
        &#39;GPP&#39;: [&#39;LUE_max&#39;, &#39;tmin0&#39;, &#39;tmin1&#39;, &#39;vpd0&#39;, &#39;vpd1&#39;],
        &#39;NPP&#39;: MOD17.required_parameters
    }
    required_drivers = {
        &#39;GPP&#39;: [&#39;fPAR&#39;, &#39;Tmin&#39;, &#39;VPD&#39;, &#39;PAR&#39;],
        &#39;NPP&#39;: [&#39;fPAR&#39;, &#39;Tmin&#39;, &#39;VPD&#39;, &#39;PAR&#39;, &#39;LAI&#39;, &#39;Tmean&#39;, &#39;years&#39;]
    }

    def compile_gpp_model(
            self, observed: Sequence, drivers: Sequence) -&gt; pm.Model:
        &#39;&#39;&#39;
        Creates a new GPP model based on the prior distribution. Model can be
        re-compiled multiple times, e.g., for cross validation.

        Parameters
        ----------
        observed : Sequence
            Sequence of observed values that will be used to calibrate the model;
            i.e., model is scored by how close its predicted values are to the
            observed values
        drivers : list or tuple
            Sequence of driver datasets to be supplied, in order, to the
            model&#39;s run function

        Returns
        -------
        pm.Model
        &#39;&#39;&#39;
        # Define the objective/ likelihood function
        log_likelihood = BlackBoxLikelihood(
            self.model, observed, x = drivers, weights = self.weights,
            objective = self.config[&#39;optimization&#39;][&#39;objective&#39;].lower())
        # With this context manager, &#34;all PyMC3 objects introduced in the indented
        #   code block...are added to the model behind the scenes.&#34;
        with pm.Model() as model:
            # (Stochstic) Priors for unknown model parameters
            LUE_max = pm.TruncatedNormal(&#39;LUE_max&#39;,
                **self.prior[&#39;LUE_max&#39;], **self.bounds[&#39;LUE_max&#39;])
            # NOTE: tmin0, vpd0 are fixed at Collection 6.1 values
            tmin0 = self.params[&#39;tmin0&#39;]
            tmin1 = pm.Uniform(&#39;tmin1&#39;, **self.bounds[&#39;tmin1&#39;])
            # NOTE: Upper bound on `vpd1` is set by the maximum observed VPD
            vpd0 = self.params[&#39;vpd0&#39;]
            vpd1 = pm.Uniform(&#39;vpd1&#39;,
                lower = self.bounds[&#39;vpd1&#39;][&#39;lower&#39;],
                upper = drivers[2].max().round(0))
            # Convert model parameters to a tensor vector
            params_list = [LUE_max, tmin0, tmin1, vpd0, vpd1]
            params = at.as_tensor_variable(params_list)
            # Key step: Define the log-likelihood as an added potential
            pm.Potential(&#39;likelihood&#39;, log_likelihood(params))
        return model

    def compile_npp_model(
            self, observed: Sequence, drivers: Sequence) -&gt; pm.Model:
        &#39;&#39;&#39;
        Creates a new NPP model based on the prior distribution. Model can be
        re-compiled multiple times, e.g., for cross validation.

        Parameters
        ----------
        observed : Sequence
            Sequence of observed values that will be used to calibrate the model;
            i.e., model is scored by how close its predicted values are to the
            observed values
        drivers : list or tuple
            Sequence of driver datasets to be supplied, in order, to the
            model&#39;s run function

        Returns
        -------
        pm.Model
        &#39;&#39;&#39;
        # Define the objective/ likelihood function
        log_likelihood = BlackBoxLikelihood(
            self.model, observed, x = drivers, weights = self.weights,
            objective = self.config[&#39;optimization&#39;][&#39;objective&#39;].lower())
        # With this context manager, &#34;all PyMC3 objects introduced in the indented
        #   code block...are added to the model behind the scenes.&#34;
        with pm.Model() as model:
            # Setting GPP parameters that are known--EXCEPT tmin1
            LUE_max = self.params[&#39;LUE_max&#39;]
            tmin0   = self.params[&#39;tmin0&#39;]
            tmin1   = self.params[&#39;tmin1&#39;]
            vpd0    = self.params[&#39;vpd0&#39;]
            vpd1    = self.params[&#39;vpd1&#39;]
            # SLA fixed at prior mean
            SLA = np.exp(self.prior[&#39;SLA&#39;][&#39;mu&#39;])
            # Allometry ratios prescribe narrow range around Collection 6.1 values
            froot_leaf_ratio = pm.Triangular(
                &#39;froot_leaf_ratio&#39;, **self.prior[&#39;froot_leaf_ratio&#39;])
            # (Stochstic) Priors for unknown model parameters
            Q10_froot = pm.TruncatedNormal(
                &#39;Q10_froot&#39;, **self.prior[&#39;Q10_froot&#39;], **self.bounds[&#39;Q10&#39;])
            leaf_mr_base = pm.LogNormal(
                &#39;leaf_mr_base&#39;, **self.prior[&#39;leaf_mr_base&#39;])
            froot_mr_base = pm.LogNormal(
                &#39;froot_mr_base&#39;, **self.prior[&#39;froot_mr_base&#39;])
            # For GRS and CRO, livewood mass and respiration are zero
            if np.equal(list(self.prior[&#39;livewood_mr_base&#39;].values()), [0, 0]).all():
                livewood_leaf_ratio = 0
                livewood_mr_base = 0
                Q10_livewood = 0
            else:
                livewood_leaf_ratio = pm.Triangular(
                    &#39;livewood_leaf_ratio&#39;, **self.prior[&#39;livewood_leaf_ratio&#39;])
                livewood_mr_base = pm.LogNormal(
                    &#39;livewood_mr_base&#39;, **self.prior[&#39;livewood_mr_base&#39;])
                Q10_livewood = pm.TruncatedNormal(
                    &#39;Q10_livewood&#39;, **self.prior[&#39;Q10_livewood&#39;],
                    **self.bounds[&#39;Q10&#39;])
            # Convert model parameters to a tensor vector
            params_list = [
                LUE_max, tmin0, tmin1, vpd0, vpd1, SLA,
                Q10_livewood, Q10_froot, froot_leaf_ratio, livewood_leaf_ratio,
                leaf_mr_base, froot_mr_base, livewood_mr_base
            ]
            params = at.as_tensor_variable(params_list)
            # Key step: Define the log-likelihood as an added potential
            pm.Potential(&#39;likelihood&#39;, log_likelihood(params))
        return model


class CalibrationAPI(object):
    &#39;&#39;&#39;
    Convenience class for calibrating the MOD17 GPP and NPP models. Meant to
    be used with `fire.Fire()`.
    &#39;&#39;&#39;
    def __init__(self, config = None):
        config_file = config
        if config_file is None:
            config_file = os.path.join(
                MOD17_DIR, &#39;data/MOD17_calibration_config.json&#39;)
        with open(config_file, &#39;r&#39;) as file:
            self.config = json.load(file)
        self.hdf5 = self.config[&#39;data&#39;][&#39;file&#39;]

    def _clean(self, raw: Sequence, drivers: Sequence, protocol: str = &#39;GPP&#39;):
        &#39;Cleans up data values according to a prescribed protocol&#39;
        if protocol == &#39;GPP&#39;:
            # Filter out observed GPP values when GPP is negative or when
            #   APAR &lt; 0.1 g C m-2 day-1
            apar = drivers[&#39;fPAR&#39;] * drivers[&#39;PAR&#39;]
            return np.where(
                apar &lt; 0.1, np.nan, np.where(raw &lt; 0, np.nan, raw))

    def _filter(self, raw: Sequence, size: int):
        &#39;Apply a smoothing filter with zero phase offset&#39;
        if size &gt; 1:
            window = np.ones(size) / size
            return np.apply_along_axis(
                lambda x: signal.filtfilt(window, np.ones(1), x), 0, raw)
        return raw # Or, revert to the raw data

    def clean_observed(
            self, raw: Sequence, drivers: Sequence, driver_labels: Sequence,
            protocol: str = &#39;GPP&#39;, filter_length: int = 2) -&gt; Sequence:
        &#39;&#39;&#39;
        Cleans observed tower flux data according to a prescribed protocol.

        - For GPP data: Removes observations where GPP &lt; 0 or where APAR is
            &lt; 0.1 MJ m-2 day-1

        Parameters
        ----------
        raw : Sequence
        drivers : Sequence
        driver_labels : Sequence
        protocol : str
        filter_length : int
            The window size for the smoothing filter, applied to the observed
            data

        Returns
        -------
        Sequence
        &#39;&#39;&#39;
        if protocol != &#39;GPP&#39;:
            raise NotImplementedError(&#39;&#34;protocol&#34; must be one of: &#34;GPP&#34;&#39;)
        # Read in the observed data and apply smoothing filter
        obs = self._filter(raw, filter_length)
        obs = self._clean(obs, dict([
            (driver_labels[i], data)
            for i, data in enumerate(drivers)
        ]), protocol = &#39;GPP&#39;)
        return obs

    def export_bplut(
            self, model: str, output_path: str, thin: int = 10,
            burn: int = 1000):
        &#39;&#39;&#39;
        Export the BPLUT using the posterior mean from the MCMC sampler. NOTE:
        The posterior mean is usually not the best estimate for poorly
        identified parameters.

        Parameters
        ----------
        model : str
            The name of the model (&#34;GPP&#34; or &#34;NPP&#34;)
        output_path : str
            The output CSV file path
        thin : int
            Thinning rate
        burn : int
            The burn-in (i.e., first N samples to discard)
        &#39;&#39;&#39;
        params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][model])
        bplut = params_dict.copy()
        # Filter the parameters to just those for the PFT of interest
        for pft in PFT_VALID:
            backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] %\
                (model, pft)
            params = dict([(k, v[pft]) for k, v in params_dict.items()])
            sampler = MOD17StochasticSampler(
                self.config, getattr(MOD17, &#39;_%s&#39; % model.lower()), params,
                backend = backend)
            trace = sampler.get_trace()
            fit = trace.sel(
                draw = slice(burn, None, thin))[&#39;posterior&#39;].mean()
            for each in MOD17.required_parameters:
                try:
                    bplut[each][pft] = float(fit[each])
                except KeyError:
                    continue
        write_bplut(bplut, output_path)

    def export_posterior(
            self, model: str, param: str, output_path: str, thin: int = 10,
            burn: int = 1000, k_folds: int = 1):
        &#39;&#39;&#39;
        Exports posterior distribution for a parameter, for each PFT to HDF5.

        Parameters
        ----------
        model : str
            The name of the model (&#34;GPP&#34; or &#34;NPP&#34;)
        param : str
            The model parameter to export
        output_path : str
            The output HDF5 file path
        thin : int
            Thinning rate
        burn : int
            The burn-in (i.e., first N samples to discard)
        k_folds : int
            The number of k-folds used in cross-calibration/validation;
            if more than one (default), the folds for each PFT will be
            combined into a single HDF5 file
        &#39;&#39;&#39;
        params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][model])
        bplut = params_dict.copy()
        # Filter the parameters to just those for the PFT of interest
        post = []
        for pft in PFT_VALID:
            params = dict([(k, v[pft]) for k, v in params_dict.items()])
            backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] %\
                (model, pft)
            post_by_fold = []
            for fold in range(1, k_folds + 1):
                if k_folds &gt; 1:
                    backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] %\
                        (f&#39;{model}-k{fold}&#39;, pft)
                sampler = MOD17StochasticSampler(
                    self.config, getattr(MOD17, &#39;_%s&#39; % model.lower()), params,
                    backend = backend)
                trace = sampler.get_trace()
                fit = trace.sel(draw = slice(burn, None, thin))[&#39;posterior&#39;]
                num_samples = fit.sizes[&#39;chain&#39;] * fit.sizes[&#39;draw&#39;]
                if param in fit:
                    post_by_fold.append(
                        az.extract_dataset(fit, combined = True)[param].values)
                else:
                    # In case there is, e.g., a parameter that takes on a
                    #   constant value for a specific PFT
                    if k_folds &gt; 1:
                        post_by_fold.append(np.ones((1, num_samples)) * np.nan)
                    else:
                        post_by_fold.append(np.ones(num_samples) * np.nan)
            if k_folds &gt; 1:
                post.append(np.vstack(post_by_fold))
            else:
                post.extend(post_by_fold)
        # If not every PFT&#39;s posterior has the same number of samples (e.g.,
        #   when one set of chains was run longer than another)...
        if not all([p.shape == post[0].shape for p in post]):
            max_len = max([p.shape for p in post])[0]
            # ...Reshape all posteriors to match the greatest sample size
            import ipdb
            ipdb.set_trace()#FIXME
            post = [
                np.pad(
                    p.astype(np.float32), (0, max_len - p.size),
                    mode = &#39;constant&#39;, constant_values = (np.nan,))
                for p in post
            ]
        with h5py.File(output_path, &#39;a&#39;) as hdf:
            post = np.stack(post)
            ts = datetime.date.today().strftime(&#39;%Y-%m-%d&#39;) # Today&#39;s date
            dataset = hdf.create_dataset(
                f&#39;{param}_posterior&#39;, post.shape, np.float32, post)
            dataset.attrs[&#39;description&#39;] = &#39;CalibrationAPI.export_posterior() on {ts}&#39;

    def export_likely_posterior(
            self, model: str, param: str, output_path: str, thin: int = 10,
            burn: int = 1000, ptile: int = 95):
        &#39;&#39;&#39;
        Exports posterior distribution for a parameter based on likelihood

        Parameters
        ----------
        model : str
            The name of the model (&#34;GPP&#34; or &#34;NPP&#34;)
        param : str
            The model parameter to export
        output_path : str
            The output HDF5 file path
        thin : int
            Thinning rate
        burn : int
            The burn-in (i.e., first N samples to discard)
        ptile : int
            The percentile cutoff for likelihood; only samples at or above
            this likelihood cutoff will be included
        &#39;&#39;&#39;
        params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][model])
        bplut = params_dict.copy()
        # Filter the parameters to just those for the PFT of interest
        post = []
        likelihood = []
        for pft in PFT_VALID:
            backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] % (model, pft)
            params = dict([(k, v[pft]) for k, v in params_dict.items()])
            sampler = MOD17StochasticSampler(
                self.config, getattr(MOD17, &#39;_%s&#39; % model.lower()), params,
                backend = backend)
            trace = sampler.get_trace()
            fit = trace.sel(draw = slice(burn, None, thin))
            # Find the likelihood value associated with the cutoff percentile
            ll = az.extract_dataset(
                fit, combined = True)[&#39;log_likelihood&#39;].values
            values = az.extract_dataset(fit, combined = True)[param].values
            cutoff = np.percentile(ll, ptile)
            post.append(values[ll &gt;= cutoff])
            likelihood.append(ll[ll &gt;= cutoff])
        with h5py.File(output_path, &#39;a&#39;) as hdf:
            n = max([len(p) for p in post])
            # Make sure all arrays are the same size
            post = np.stack([
                np.concatenate((p, np.full((n - len(p),), np.nan)))
                for p in post
            ])
            likelihood = np.stack([
                np.concatenate((p, np.full((n - len(p),), np.nan)))
                for p in likelihood
            ])
            ts = datetime.date.today().strftime(&#39;%Y-%m-%d&#39;) # Today&#39;s date
            dataset = hdf.create_dataset(
                f&#39;{param}_posterior&#39;, post.shape, np.float32, post)
            dataset.attrs[&#39;description&#39;] = &#39;CalibrationAPI.export_likely_posterior() on {ts}&#39;
            dataset = hdf.create_dataset(
                f&#39;{param}_likelihood&#39;, likelihood.shape, np.float32, likelihood)

    def tune_gpp(
            self, pft: int, plot_trace: bool = False, ipdb: bool = False,
            save_fig: bool = False, **kwargs):
        &#39;&#39;&#39;
        Run the MOD17 GPP calibration.

        Parameters
        ----------
        pft : int
            The Plant Functional Type (PFT) to calibrate
        plot_trace : bool
            True to plot the trace for a previous calibration run; this will
            also NOT start a new calibration (Default: False)
        ipdb : bool
            True to drop the user into an ipdb prompt, prior to and instead of
            running calibration
        save_fig : bool
            True to save figures to files instead of showing them
            (Default: False)
        **kwargs
            Additional keyword arguments passed to
            `MOD17StochasticSampler.run()`
        &#39;&#39;&#39;
        assert pft in PFT_VALID, f&#39;Invalid PFT: {pft}&#39;
        # Pass configuration parameters to MOD17StochasticSampler.run()
        for key in (&#39;chains&#39;, &#39;draws&#39;, &#39;tune&#39;, &#39;scaling&#39;):
            if key in self.config[&#39;optimization&#39;].keys() and not key in kwargs.keys():
                kwargs[key] = self.config[&#39;optimization&#39;][key]
        # Filter the parameters to just those for the PFT of interest
        params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][&#39;GPP&#39;])
        # Load blacklisted sites (if any)
        blacklist = self.config[&#39;data&#39;][&#39;sites_blacklisted&#39;]
        params_dict = dict([(k, v[pft]) for k, v in params_dict.items()])
        model = MOD17(params_dict)
        objective = self.config[&#39;optimization&#39;][&#39;objective&#39;].lower()

        print(&#39;Loading driver datasets...&#39;)
        with h5py.File(self.hdf5, &#39;r&#39;) as hdf:
            sites = hdf[&#39;FLUXNET/site_id&#39;][:]
            if hasattr(sites[0], &#39;decode&#39;):
                sites = list(map(lambda x: x.decode(&#39;utf-8&#39;), sites))
            # Get dominant PFT
            pft_map = pft_dominant(hdf[&#39;state/PFT&#39;][:], site_list = sites)
            # Blacklist various sites
            pft_mask = np.logical_and(pft_map == pft, ~np.in1d(sites, blacklist))
            # NOTE: Converting from Kelvin to Celsius
            tday = hdf[&#39;MERRA2/T10M_daytime&#39;][:][:,pft_mask] - 273.15
            qv10m = hdf[&#39;MERRA2/QV10M_daytime&#39;][:][:,pft_mask]
            ps = hdf[&#39;MERRA2/PS_daytime&#39;][:][:,pft_mask]
            drivers = [ # fPAR, Tmin, VPD, PAR
                hdf[&#39;MODIS/MOD15A2HGF_fPAR_interp&#39;][:][:,pft_mask],
                hdf[&#39;MERRA2/Tmin&#39;][:][:,pft_mask] - 273.15,
                MOD17.vpd(qv10m, ps, tday),
                MOD17.par(hdf[&#39;MERRA2/SWGDN&#39;][:][:,pft_mask]),
            ]
            # Convert fPAR from (%) to [0,1]
            drivers[0] = np.nanmean(drivers[0], axis = -1) / 100
            # If RMSE is used, then we want to pay attention to weighting
            weights = None
            if objective in (&#39;rmsd&#39;, &#39;rmse&#39;):
                weights = hdf[&#39;weights&#39;][pft_mask][np.newaxis,:]\
                    .repeat(tday.shape[0], axis = 0)
            # Check that driver data do not contain NaNs
            for d, each in enumerate(drivers):
                name = (&#39;fPAR&#39;, &#39;Tmin&#39;, &#39;VPD&#39;, &#39;PAR&#39;)[d]
                assert not np.isnan(each).any(),\
                    f&#39;Driver dataset &#34;{name}&#34; contains NaNs&#39;
            tower_gpp = hdf[&#39;FLUXNET/GPP&#39;][:][:,pft_mask]
            # Read the validation mask; mask out observations that are
            #   reserved for validation
            print(&#39;Masking out validation data...&#39;)
            mask = hdf[&#39;FLUXNET/validation_mask&#39;][pft]
            tower_gpp[mask] = np.nan

        # Clean observations, then mask out driver data where the are no
        #   observations
        tower_gpp = self.clean_observed(
            tower_gpp, drivers, MOD17StochasticSampler.required_drivers[&#39;GPP&#39;],
            protocol = &#39;GPP&#39;)
        if weights is not None:
            weights = weights[~np.isnan(tower_gpp)]
        for d, _ in enumerate(drivers):
            drivers[d] = drivers[d][~np.isnan(tower_gpp)]
        tower_gpp = tower_gpp[~np.isnan(tower_gpp)]

        print(&#39;Initializing sampler...&#39;)
        backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] % (&#39;GPP&#39;, pft)
        sampler = MOD17StochasticSampler(
            self.config, MOD17._gpp, params_dict, backend = backend,
            weights = weights)
        if plot_trace or ipdb:
            if ipdb:
                import ipdb
                ipdb.set_trace()
            trace = sampler.get_trace()
            az.plot_trace(trace, var_names = MOD17.required_parameters[0:5])
            pyplot.show()
            return
        # Get (informative) priors for just those parameters that have them
        with open(self.config[&#39;optimization&#39;][&#39;prior&#39;], &#39;r&#39;) as file:
            prior = json.load(file)
        prior_params = filter(
            lambda p: p in prior.keys(), sampler.required_parameters[&#39;GPP&#39;])
        prior = dict([
            (p, {&#39;mu&#39;: prior[p][&#39;mu&#39;][pft], &#39;sigma&#39;: prior[p][&#39;sigma&#39;][pft]})
            for p in prior_params
        ])
        sampler.run(
            tower_gpp, drivers, prior = prior, save_fig = save_fig, **kwargs)

    def tune_npp(
            self, pft: int, plot_trace: bool = False, ipdb: bool = False,
            save_fig: bool = False, climatology = False,
            cutoff: Number = 2385, k_folds: int = 1, **kwargs):
        r&#39;&#39;&#39;
        Run the MOD17 NPP calibration. If k-folds cross-validation is used,
        the model is calibrated on $k$ random subsets of the data and a
        series of file is created, e.g., as:

            MOD17_NPP_calibration_PFT1.h5
            MOD17_NPP-k1_calibration_PFT1.nc4
            MOD17_NPP-k2_calibration_PFT1.nc4
            ...

        Where each `.nc4` file is a standard `arviz` backend and the `.h5`
        indicates which indices from the NPP observations vector, after
        removing NaNs, were excluded (i.e., the indices of the test data).

        Parameters
        ----------
        pft : int
            The Plant Functional Type (PFT) to calibrate
        plot_trace : bool
            True to display the trace plot ONLY and not run calibration
            (Default: False)
        ipdb : bool
            True to drop into an interactive Python debugger (`ipdb`) after
            loading an existing trace (Default: False)
        save_fig : bool
            True to save the post-calibration trace plot to a file instead of
            displaying it (Default: False)
        climatology : bool
            True to use a MERRA-2 climatology (and look for it in the drivers
            file), i.e., use `MERRA2_climatology` group instead of
            `surface_met_MERRA2` group (Default: False)
        cutoff : Number
            Maximum value of observed NPP (g C m-2 year-1); values above this
            cutoff will be discarded and not used in calibration
            (Default: 2385)
        k_folds : int
            Number of folds to use in k-folds cross-validation; defaults to
            k=1, i.e., no cross-validation is performed.
        **kwargs
            Additional keyword arguments passed to
            `MOD17StochasticSampler.run()`
        &#39;&#39;&#39;
        assert pft in PFT_VALID, f&#39;Invalid PFT: {pft}&#39;
        prefix = &#39;MERRA2_climatology&#39; if climatology else &#39;surface_met_MERRA2&#39;
        params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][&#39;NPP&#39;])
        # Filter the parameters to just those for the PFT of interest
        params_dict = dict([(k, v[pft]) for k, v in params_dict.items()])
        model = MOD17(params_dict)
        kwargs.update({&#39;var_names&#39;: [
            &#39;~LUE_max&#39;, &#39;~tmin0&#39;, &#39;~tmin1&#39;, &#39;~vpd0&#39;, &#39;~vpd1&#39;, &#39;~log_likelihood&#39;
        ]})
        # Pass configuration parameters to MOD17StochasticSampler.run()
        for key in (&#39;chains&#39;, &#39;draws&#39;, &#39;tune&#39;, &#39;scaling&#39;):
            if key in self.config[&#39;optimization&#39;].keys():
                kwargs[key] = self.config[&#39;optimization&#39;][key]
        print(&#39;Loading driver datasets...&#39;)
        with h5py.File(self.hdf5, &#39;r&#39;) as hdf:
            # NOTE: This is only recorded at the site-level; no need to
            #   determine modal PFT across subgrid
            pft_map = hdf[&#39;NPP/PFT&#39;][:]
            # Leave out sites where there is no fPAR (and no LAI) data
            fpar = hdf[&#39;NPP/MOD15A2H_fPAR_clim_filt&#39;][:]
            mask = np.logical_and(
                    pft_map == pft, ~np.isnan(np.nanmean(fpar, axis = -1))\
                .all(axis = 0))
            # NOTE: Converting from Kelvin to Celsius
            tday = hdf[f&#39;NPP/{prefix}/T10M_daytime&#39;][:][:,mask] - 273.15
            qv10m = hdf[f&#39;NPP/{prefix}/QV10M_daytime&#39;][:][:,mask]
            ps = hdf[f&#39;NPP/{prefix}/PS_daytime&#39;][:][:,mask]
            drivers = [ # fPAR, Tmin, VPD, PAR, LAI, Tmean, years
                hdf[&#39;NPP/MOD15A2H_fPAR_clim_filt&#39;][:][:,mask],
                hdf[f&#39;NPP/{prefix}/Tmin&#39;][:][:,mask]  - 273.15,
                MOD17.vpd(qv10m, ps, tday),
                MOD17.par(hdf[f&#39;NPP/{prefix}/SWGDN&#39;][:][:,mask]),
                hdf[&#39;NPP/MOD15A2H_LAI_clim_filt&#39;][:][:,mask],
                hdf[f&#39;NPP/{prefix}/T10M&#39;][:][:,mask] - 273.15,
                np.full(ps.shape, 1) # i.e., A 365-day climatological year (&#34;Year 1&#34;)
            ]
            observed_npp = hdf[&#39;NPP/NPP_total&#39;][:][mask]
        if cutoff is not None:
            observed_npp[observed_npp &gt; cutoff] = np.nan
        # Set negative VPD to zero
        drivers[2] = np.where(drivers[2] &lt; 0, 0, drivers[2])
        # Convert fPAR from (%) to [0,1] and re-scale LAI; reshape fPAR, LAI
        drivers[0] = np.nanmean(drivers[0], axis = -1) * 0.01
        drivers[4] = np.nanmean(drivers[4], axis = -1) * 0.1
        # TODO Mask out driver data where the are no observations
        for d, _ in enumerate(drivers):
            drivers[d] = drivers[d][:,~np.isnan(observed_npp)]
        observed_npp = observed_npp[~np.isnan(observed_npp)]

        if k_folds &gt; 1:
            # Back-up the original (complete) datasets
            _drivers = [d.copy() for d in drivers]
            _observed_npp = observed_npp.copy()
            # Randomize the indices of the NPP data
            indices = np.arange(0, observed_npp.size)
            np.random.shuffle(indices)
            # Get the starting and ending index of each fold
            fold_idx = np.array([indices.size // k_folds] * k_folds) * np.arange(0, k_folds)
            fold_idx = list(map(list, zip(fold_idx, fold_idx + indices.size // k_folds)))
            # Ensure that the entire dataset is used
            fold_idx[-1][-1] = indices.max()
            idx_test = [indices[start:end] for start, end in fold_idx]

        # Loop over each fold (or the entire dataset, if num. folds == 1)
        for k, fold in enumerate(range(1, k_folds + 1)):
            backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] % (&#39;NPP&#39;, pft)
            if k_folds &gt; 1 and fold == 1:
                # Create an HDF5 file with the same name as the (original)
                #   netCDF4 back-end, store the test indices
                with h5py.File(backend.replace(&#39;nc4&#39;, &#39;h5&#39;), &#39;w&#39;) as hdf:
                    out = list(idx_test)
                    size = indices.size // k_folds
                    try:
                        out = np.stack(out)
                    except ValueError:
                        size = max((o.size for o in out))
                        for i in range(0, len(out)):
                            out[i] = np.concatenate((out[i], [np.nan] * (size - out[i].size)))
                    hdf.create_dataset(
                        &#39;test_indices&#39;, (k_folds, size), np.int32, np.stack(out))
                # Restore the original NPP dataset
                observed_npp = _observed_npp.copy()
                # Set to NaN all the test indices
                idx = idx_test[k]
                observed_npp[idx] = np.nan
                # Same for drivers, after restoring from the original
                drivers = [d.copy()[:,~np.isnan(observed_npp)] for d in _drivers]
                observed_npp = observed_npp[~np.isnan(observed_npp)]
            # Use a different naming scheme for the backend
            if k_folds &gt; 1:
                backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] % (f&#39;NPP-k{fold}&#39;, pft)

            print(&#39;Initializing sampler...&#39;)
            sampler = MOD17StochasticSampler(
                self.config, MOD17._npp, params_dict, backend = backend,
                model_name = &#39;NPP&#39;)
            if plot_trace or ipdb:
                if ipdb:
                    import ipdb
                    ipdb.set_trace()
                trace = sampler.get_trace()
                az.plot_trace(
                    trace, var_names = MOD17.required_parameters[5:])
                pyplot.show()
                return
            # Get (informative) priors for just those parameters that have them
            with open(self.config[&#39;optimization&#39;][&#39;prior&#39;], &#39;r&#39;) as file:
                prior = json.load(file)
            prior_params = filter(
                lambda p: p in prior.keys(), sampler.required_parameters[&#39;NPP&#39;])
            prior = dict([
                (p, prior[p]) for p in prior_params
            ])
            for key in prior.keys():
                # And make sure to subset to the chosen PFT!
                for arg in prior[key].keys():
                    prior[key][arg] = prior[key][arg][pft]
            sampler.run(
                observed_npp, drivers, prior = prior, save_fig = save_fig,
                **kwargs)


if __name__ == &#39;__main__&#39;:
    import fire
    with warnings.catch_warnings():
        warnings.simplefilter(&#39;ignore&#39;)
        fire.Fire(CalibrationAPI)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mod17.calibration.AbstractSampler"><code class="flex name class">
<span>class <span class="ident">AbstractSampler</span></span>
</code></dt>
<dd>
<div class="desc"><p>Generic algorithm for fitting a model to data based on observed values
similar to what we can produce with our model. Not intended to be called
directly.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractSampler(object):
    &#39;&#39;&#39;
    Generic algorithm for fitting a model to data based on observed values
    similar to what we can produce with our model. Not intended to be called
    directly.
    &#39;&#39;&#39;

    def get_posterior(self, thin: int = 1) -&gt; np.ndarray:
        &#39;&#39;&#39;
        Returns a stacked posterior array, with optional thinning, combining
        all the chains together.

        Parameters
        ----------
        thin : int

        Returns
        -------
        numpy.ndarray
        &#39;&#39;&#39;
        trace = az.from_netcdf(self.backend)
        return np.stack([ # i.e., get every ith element, each chain
            trace[&#39;posterior&#39;][p].values[:,::thin].ravel()
            for p in self.required_parameters[self.name]
        ], axis = -1)

    def get_trace(
            self, thin: int = None, burn: int = None
        ) -&gt; az.data.inference_data.InferenceData:
        &#39;&#39;&#39;
        Extracts the trace from the backend data store.

        Parameters
        ----------
        thin : int
            Thinning rate
        burn : int
            The burn-in (i.e., first N samples to discard)
        &#39;&#39;&#39;
        trace = az.from_netcdf(self.backend)
        if thin is None and burn is None:
            return trace
        return trace.sel(draw = slice(burn, None, thin))

    def plot_autocorr(self, thin: int = None, burn: int = None, **kwargs):
        &#39;&#39;&#39;
        Auto-correlation plot for an MCMC sample.

        Parameters
        ----------
        thin : int
            Thinning rate
        burn : int
            The burn-in (i.e., first N samples to discard)
        **kwargs
            Additional keyword arguments to `arviz.plot_autocorr()`.
        &#39;&#39;&#39;
        assert os.path.exists(self.backend),\
            &#39;Could not find file backend!&#39;
        trace = az.from_netcdf(self.backend)
        kwargs.setdefault(&#39;combined&#39;, True)
        if thin is None:
            az.plot_autocorr(trace, **kwargs)
        else:
            burn = 0 if burn is None else burn
            az.plot_autocorr(
                trace.sel(draw = slice(burn, None, thin))[&#39;posterior&#39;],
                **kwargs)
        pyplot.show()

    def plot_forest(self, **kwargs):
        &#39;&#39;&#39;
        Forest plot for an MCMC sample.

        In particular:

        - `hdi_prob`: A float indicating the highest density interval (HDF) to
            plot
        &#39;&#39;&#39;
        assert os.path.exists(self.backend),\
            &#39;Could not find file backend!&#39;
        trace = az.from_netcdf(self.backend)
        az.plot_forest(trace, **kwargs)
        pyplot.show()

    def plot_pair(self, **kwargs):
        &#39;&#39;&#39;
        Paired variables plot for an MCMC sample.

        Parameters
        ----------
        **kwargs
            Additional keyword arguments to `arviz.plot_pair()`.
        &#39;&#39;&#39;
        assert os.path.exists(self.backend),\
            &#39;Could not find file backend!&#39;
        trace = az.from_netcdf(self.backend)
        az.plot_pair(trace, **kwargs)
        pyplot.show()

    def plot_partial_score(
            self, observed: Sequence, drivers: Sequence, fit: dict = None):
        &#39;&#39;&#39;
        Plots the &#34;partial effect&#34; of a single parameter: the score of the
        model at that parameter&#39;s current value against a sweep of possible
        parameter values. All other parameters are held fixed at the best-fit
        values.

        Parameters
        ----------
        observed : Sequence
            Sequence of observed values that will be used to calibrate the model;
            i.e., model is scored by how close its predicted values are to the
            observed values
        drivers : list or tuple
            Sequence of driver datasets to be supplied, in order, to the
            model&#39;s run function
        fit : dict or None
            The best-fit parameter values used for those parameters that are
            fixed
        &#39;&#39;&#39;
        trace = az.from_netcdf(self.backend)
        if fit is None:
            # Mean of posterior are &#34;best fit&#34; values
            fit = trace[&#39;posterior&#39;].mean()
        fit_params = list(filter(
            lambda p: p in fit, self.required_parameters[self.name]))
        # The NPP model depends on constants not included in the fit
        constants = []
        if self.name == &#39;NPP&#39;:
            constants = [
                self.params[p]
                for p in [&#39;LUE_max&#39;, &#39;tmin0&#39;, &#39;tmin1&#39;, &#39;vpd0&#39;, &#39;vpd1&#39;]
            ]
        n = len(fit_params)
        nrow = n
        ncol = 1
        if n &gt; 4:
            nrow = 2
            ncol = n - (n // 2)
        fig, axes = pyplot.subplots(
            nrow, ncol, figsize = (n * 2, n), sharey = True)
        i = 0
        for j in range(nrow):
            for k in range(ncol):
                if i &gt;= n:
                    break
                free_param = fit_params[i]
                fixed = np.stack([
                    fit[p].values for p in fit_params
                ])[np.newaxis,:].repeat(30, axis = 0)
                sweep = np.linspace(
                    trace[&#39;posterior&#39;][free_param].min(),
                    trace[&#39;posterior&#39;][free_param].max(), num = 30)
                fixed[:,i] = sweep
                # Need to concatenate GPP parameters at begining of fixed
                scores = -np.array(self.score_posterior(
                    observed, drivers, [
                        [*constants, *f] for f in fixed.tolist()
                    ]))
                axes[j,k].plot(sweep, scores, &#39;k-&#39;)
                axes[j,k].axvline(
                    fit[free_param], color = &#39;red&#39;, linestyle = &#39;dashed&#39;,
                    label = &#39;Posterior Mean&#39;)
                axes[j,k].set_xlabel(free_param)
                axes[j,k].set_title(free_param)
                axes[j,k].legend()
                i += 1
        # Delete the last empty subplot
        if n % 2 != 0:
            fig.delaxes(axes.flatten()[-1])
        axes[0, 0].set_ylabel(&#39;Score&#39;)
        pyplot.tight_layout()
        pyplot.show()

    def plot_posterior(self, **kwargs):
        &#39;&#39;&#39;
        Plots the posterior distribution for an MCMC sample.

        Parameters
        ----------
        **kwargs
            Additional keyword arguments to `arviz.plot_posterior()`.
        &#39;&#39;&#39;
        assert os.path.exists(self.backend),\
            &#39;Could not find file backend!&#39;
        trace = az.from_netcdf(self.backend)
        az.plot_posterior(trace, **kwargs)
        pyplot.show()

    def score_posterior(
            self, observed: Sequence, drivers: Sequence, posterior: Sequence,
            method: str = &#39;rmsd&#39;) -&gt; Number:
        &#39;&#39;&#39;
        Returns a goodness-of-fit score based on the existing calibration.

        Parameters
        ----------
        observed : Sequence
            Sequence of observed values that will be used to calibrate the model;
            i.e., model is scored by how close its predicted values are to the
            observed values
        drivers : list or tuple
            Sequence of driver datasets to be supplied, in order, to the
            model&#39;s run function
        posterior : list or tuple
            Sequence of posterior parameter sets (i.e., nested sequence); each
            nested sequence will be scored
        method : str
            The method for generating a goodness-of-git score
            (Default: &#34;rmsd&#34;)

        Returns
        -------
        float
        &#39;&#39;&#39;
        if method != &#39;rmsd&#39;:
            raise NotImplementedError(&#39;&#34;method&#34; must be one of: &#34;rmsd&#34;&#39;)
        score_func = partial(
            rmsd, func = self.model, observed = observed, drivers = drivers)
        with get_context(&#39;spawn&#39;).Pool() as pool:
            scores = pool.map(score_func, posterior)
        return scores</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mod17.calibration.StochasticSampler" href="#mod17.calibration.StochasticSampler">StochasticSampler</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mod17.calibration.AbstractSampler.get_posterior"><code class="name flex">
<span>def <span class="ident">get_posterior</span></span>(<span>self, thin: int = 1) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a stacked posterior array, with optional thinning, combining
all the chains together.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>thin</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_posterior(self, thin: int = 1) -&gt; np.ndarray:
    &#39;&#39;&#39;
    Returns a stacked posterior array, with optional thinning, combining
    all the chains together.

    Parameters
    ----------
    thin : int

    Returns
    -------
    numpy.ndarray
    &#39;&#39;&#39;
    trace = az.from_netcdf(self.backend)
    return np.stack([ # i.e., get every ith element, each chain
        trace[&#39;posterior&#39;][p].values[:,::thin].ravel()
        for p in self.required_parameters[self.name]
    ], axis = -1)</code></pre>
</details>
</dd>
<dt id="mod17.calibration.AbstractSampler.get_trace"><code class="name flex">
<span>def <span class="ident">get_trace</span></span>(<span>self, thin: int = None, burn: int = None) ‑> arviz.data.inference_data.InferenceData</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts the trace from the backend data store.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>thin</code></strong> :&ensp;<code>int</code></dt>
<dd>Thinning rate</dd>
<dt><strong><code>burn</code></strong> :&ensp;<code>int</code></dt>
<dd>The burn-in (i.e., first N samples to discard)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_trace(
        self, thin: int = None, burn: int = None
    ) -&gt; az.data.inference_data.InferenceData:
    &#39;&#39;&#39;
    Extracts the trace from the backend data store.

    Parameters
    ----------
    thin : int
        Thinning rate
    burn : int
        The burn-in (i.e., first N samples to discard)
    &#39;&#39;&#39;
    trace = az.from_netcdf(self.backend)
    if thin is None and burn is None:
        return trace
    return trace.sel(draw = slice(burn, None, thin))</code></pre>
</details>
</dd>
<dt id="mod17.calibration.AbstractSampler.plot_autocorr"><code class="name flex">
<span>def <span class="ident">plot_autocorr</span></span>(<span>self, thin: int = None, burn: int = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Auto-correlation plot for an MCMC sample.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>thin</code></strong> :&ensp;<code>int</code></dt>
<dd>Thinning rate</dd>
<dt><strong><code>burn</code></strong> :&ensp;<code>int</code></dt>
<dd>The burn-in (i.e., first N samples to discard)</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments to <code>arviz.plot_autocorr()</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_autocorr(self, thin: int = None, burn: int = None, **kwargs):
    &#39;&#39;&#39;
    Auto-correlation plot for an MCMC sample.

    Parameters
    ----------
    thin : int
        Thinning rate
    burn : int
        The burn-in (i.e., first N samples to discard)
    **kwargs
        Additional keyword arguments to `arviz.plot_autocorr()`.
    &#39;&#39;&#39;
    assert os.path.exists(self.backend),\
        &#39;Could not find file backend!&#39;
    trace = az.from_netcdf(self.backend)
    kwargs.setdefault(&#39;combined&#39;, True)
    if thin is None:
        az.plot_autocorr(trace, **kwargs)
    else:
        burn = 0 if burn is None else burn
        az.plot_autocorr(
            trace.sel(draw = slice(burn, None, thin))[&#39;posterior&#39;],
            **kwargs)
    pyplot.show()</code></pre>
</details>
</dd>
<dt id="mod17.calibration.AbstractSampler.plot_forest"><code class="name flex">
<span>def <span class="ident">plot_forest</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Forest plot for an MCMC sample.</p>
<p>In particular:</p>
<ul>
<li><code>hdi_prob</code>: A float indicating the highest density interval (HDF) to
plot</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_forest(self, **kwargs):
    &#39;&#39;&#39;
    Forest plot for an MCMC sample.

    In particular:

    - `hdi_prob`: A float indicating the highest density interval (HDF) to
        plot
    &#39;&#39;&#39;
    assert os.path.exists(self.backend),\
        &#39;Could not find file backend!&#39;
    trace = az.from_netcdf(self.backend)
    az.plot_forest(trace, **kwargs)
    pyplot.show()</code></pre>
</details>
</dd>
<dt id="mod17.calibration.AbstractSampler.plot_pair"><code class="name flex">
<span>def <span class="ident">plot_pair</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Paired variables plot for an MCMC sample.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments to <code>arviz.plot_pair()</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_pair(self, **kwargs):
    &#39;&#39;&#39;
    Paired variables plot for an MCMC sample.

    Parameters
    ----------
    **kwargs
        Additional keyword arguments to `arviz.plot_pair()`.
    &#39;&#39;&#39;
    assert os.path.exists(self.backend),\
        &#39;Could not find file backend!&#39;
    trace = az.from_netcdf(self.backend)
    az.plot_pair(trace, **kwargs)
    pyplot.show()</code></pre>
</details>
</dd>
<dt id="mod17.calibration.AbstractSampler.plot_partial_score"><code class="name flex">
<span>def <span class="ident">plot_partial_score</span></span>(<span>self, observed: Sequence[+T_co], drivers: Sequence[+T_co], fit: dict = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Plots the "partial effect" of a single parameter: the score of the
model at that parameter's current value against a sweep of possible
parameter values. All other parameters are held fixed at the best-fit
values.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>observed</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>Sequence of observed values that will be used to calibrate the model;
i.e., model is scored by how close its predicted values are to the
observed values</dd>
<dt><strong><code>drivers</code></strong> :&ensp;<code>list</code> or <code>tuple</code></dt>
<dd>Sequence of driver datasets to be supplied, in order, to the
model's run function</dd>
<dt><strong><code>fit</code></strong> :&ensp;<code>dict</code> or <code>None</code></dt>
<dd>The best-fit parameter values used for those parameters that are
fixed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_partial_score(
        self, observed: Sequence, drivers: Sequence, fit: dict = None):
    &#39;&#39;&#39;
    Plots the &#34;partial effect&#34; of a single parameter: the score of the
    model at that parameter&#39;s current value against a sweep of possible
    parameter values. All other parameters are held fixed at the best-fit
    values.

    Parameters
    ----------
    observed : Sequence
        Sequence of observed values that will be used to calibrate the model;
        i.e., model is scored by how close its predicted values are to the
        observed values
    drivers : list or tuple
        Sequence of driver datasets to be supplied, in order, to the
        model&#39;s run function
    fit : dict or None
        The best-fit parameter values used for those parameters that are
        fixed
    &#39;&#39;&#39;
    trace = az.from_netcdf(self.backend)
    if fit is None:
        # Mean of posterior are &#34;best fit&#34; values
        fit = trace[&#39;posterior&#39;].mean()
    fit_params = list(filter(
        lambda p: p in fit, self.required_parameters[self.name]))
    # The NPP model depends on constants not included in the fit
    constants = []
    if self.name == &#39;NPP&#39;:
        constants = [
            self.params[p]
            for p in [&#39;LUE_max&#39;, &#39;tmin0&#39;, &#39;tmin1&#39;, &#39;vpd0&#39;, &#39;vpd1&#39;]
        ]
    n = len(fit_params)
    nrow = n
    ncol = 1
    if n &gt; 4:
        nrow = 2
        ncol = n - (n // 2)
    fig, axes = pyplot.subplots(
        nrow, ncol, figsize = (n * 2, n), sharey = True)
    i = 0
    for j in range(nrow):
        for k in range(ncol):
            if i &gt;= n:
                break
            free_param = fit_params[i]
            fixed = np.stack([
                fit[p].values for p in fit_params
            ])[np.newaxis,:].repeat(30, axis = 0)
            sweep = np.linspace(
                trace[&#39;posterior&#39;][free_param].min(),
                trace[&#39;posterior&#39;][free_param].max(), num = 30)
            fixed[:,i] = sweep
            # Need to concatenate GPP parameters at begining of fixed
            scores = -np.array(self.score_posterior(
                observed, drivers, [
                    [*constants, *f] for f in fixed.tolist()
                ]))
            axes[j,k].plot(sweep, scores, &#39;k-&#39;)
            axes[j,k].axvline(
                fit[free_param], color = &#39;red&#39;, linestyle = &#39;dashed&#39;,
                label = &#39;Posterior Mean&#39;)
            axes[j,k].set_xlabel(free_param)
            axes[j,k].set_title(free_param)
            axes[j,k].legend()
            i += 1
    # Delete the last empty subplot
    if n % 2 != 0:
        fig.delaxes(axes.flatten()[-1])
    axes[0, 0].set_ylabel(&#39;Score&#39;)
    pyplot.tight_layout()
    pyplot.show()</code></pre>
</details>
</dd>
<dt id="mod17.calibration.AbstractSampler.plot_posterior"><code class="name flex">
<span>def <span class="ident">plot_posterior</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Plots the posterior distribution for an MCMC sample.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments to <code>arviz.plot_posterior()</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_posterior(self, **kwargs):
    &#39;&#39;&#39;
    Plots the posterior distribution for an MCMC sample.

    Parameters
    ----------
    **kwargs
        Additional keyword arguments to `arviz.plot_posterior()`.
    &#39;&#39;&#39;
    assert os.path.exists(self.backend),\
        &#39;Could not find file backend!&#39;
    trace = az.from_netcdf(self.backend)
    az.plot_posterior(trace, **kwargs)
    pyplot.show()</code></pre>
</details>
</dd>
<dt id="mod17.calibration.AbstractSampler.score_posterior"><code class="name flex">
<span>def <span class="ident">score_posterior</span></span>(<span>self, observed: Sequence[+T_co], drivers: Sequence[+T_co], posterior: Sequence[+T_co], method: str = 'rmsd') ‑> numbers.Number</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a goodness-of-fit score based on the existing calibration.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>observed</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>Sequence of observed values that will be used to calibrate the model;
i.e., model is scored by how close its predicted values are to the
observed values</dd>
<dt><strong><code>drivers</code></strong> :&ensp;<code>list</code> or <code>tuple</code></dt>
<dd>Sequence of driver datasets to be supplied, in order, to the
model's run function</dd>
<dt><strong><code>posterior</code></strong> :&ensp;<code>list</code> or <code>tuple</code></dt>
<dd>Sequence of posterior parameter sets (i.e., nested sequence); each
nested sequence will be scored</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>str</code></dt>
<dd>The method for generating a goodness-of-git score
(Default: "rmsd")</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score_posterior(
        self, observed: Sequence, drivers: Sequence, posterior: Sequence,
        method: str = &#39;rmsd&#39;) -&gt; Number:
    &#39;&#39;&#39;
    Returns a goodness-of-fit score based on the existing calibration.

    Parameters
    ----------
    observed : Sequence
        Sequence of observed values that will be used to calibrate the model;
        i.e., model is scored by how close its predicted values are to the
        observed values
    drivers : list or tuple
        Sequence of driver datasets to be supplied, in order, to the
        model&#39;s run function
    posterior : list or tuple
        Sequence of posterior parameter sets (i.e., nested sequence); each
        nested sequence will be scored
    method : str
        The method for generating a goodness-of-git score
        (Default: &#34;rmsd&#34;)

    Returns
    -------
    float
    &#39;&#39;&#39;
    if method != &#39;rmsd&#39;:
        raise NotImplementedError(&#39;&#34;method&#34; must be one of: &#34;rmsd&#34;&#39;)
    score_func = partial(
        rmsd, func = self.model, observed = observed, drivers = drivers)
    with get_context(&#39;spawn&#39;).Pool() as pool:
        scores = pool.map(score_func, posterior)
    return scores</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="mod17.calibration.BlackBoxLikelihood"><code class="flex name class">
<span>class <span class="ident">BlackBoxLikelihood</span></span>
<span>(</span><span>model: Callable, observed: Sequence[+T_co], x: Sequence[+T_co] = None, weights: Sequence[+T_co] = None, objective: str = 'rmsd')</span>
</code></dt>
<dd>
<div class="desc"><p>A custom Theano operator that calculates the "likelihood" of model
parameters; it takes a vector of values (the parameters that define our
model) and returns a single "scalar" value (the log-likelihood).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>Callable</code></dt>
<dd>An arbitrary "black box" function that takes two arguments: the
model parameters ("params") and the forcing data ("x")</dd>
<dt><strong><code>observed</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The "observed" data that our log-likelihood function takes in</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.ndarray</code> or <code>None</code></dt>
<dd>The forcing data (input drivers) that our model requires, or None
if no driver data are required</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>Sequence</code> or <code>None</code></dt>
<dd>Optional sequence of weights applied to the model residuals (as in
weighted least squares)</dd>
<dt><strong><code>objective</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the objective (or "loss") function to use, one of
('rmsd', 'gaussian', 'kge'); defaults to "rmsd"</dd>
</dl>
<p>Initialise the Op with various things that our log-likelihood function
requires. The observed data ("observed") and drivers ("x") must be
stored on the instance so the Theano Op can work seamlessly.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BlackBoxLikelihood(at.Op):
    &#39;&#39;&#39;
    A custom Theano operator that calculates the &#34;likelihood&#34; of model
    parameters; it takes a vector of values (the parameters that define our
    model) and returns a single &#34;scalar&#34; value (the log-likelihood).

    Parameters
    ----------
    model : Callable
        An arbitrary &#34;black box&#34; function that takes two arguments: the
        model parameters (&#34;params&#34;) and the forcing data (&#34;x&#34;)
    observed : numpy.ndarray
        The &#34;observed&#34; data that our log-likelihood function takes in
    x : numpy.ndarray or None
        The forcing data (input drivers) that our model requires, or None
        if no driver data are required
    weights : Sequence or None
        Optional sequence of weights applied to the model residuals (as in
        weighted least squares)
    objective : str
        Name of the objective (or &#34;loss&#34;) function to use, one of
        (&#39;rmsd&#39;, &#39;gaussian&#39;, &#39;kge&#39;); defaults to &#34;rmsd&#34;
    &#39;&#39;&#39;
    itypes = [at.dvector] # Expects a vector of parameter values when called
    otypes = [at.dscalar] # Outputs a single scalar value (the log likelihood)

    def __init__(
            self, model: Callable, observed: Sequence, x: Sequence = None,
            weights: Sequence = None, objective: str = &#39;rmsd&#39;):
        &#39;&#39;&#39;
        Initialise the Op with various things that our log-likelihood function
        requires. The observed data (&#34;observed&#34;) and drivers (&#34;x&#34;) must be
        stored on the instance so the Theano Op can work seamlessly.
        &#39;&#39;&#39;
        self.model = model
        self.observed = observed
        self.x = x
        self.weights = weights
        if objective in (&#39;rmsd&#39;, &#39;rmse&#39;):
            self._loglik = self.loglik
        elif objective == &#39;gaussian&#39;:
            self._loglik = self.loglik_gaussian
        elif objective == &#39;kge&#39;:
            self._loglik = self.loglik_kge
        else:
            raise ValueError(&#39;Unknown &#34;objective&#34; function specified&#39;)

    def loglik(
            self, params: Sequence, observed: Sequence,
            x: Sequence = None) -&gt; Number:
        &#39;&#39;&#39;
        Pseudo-log likelihood, based on the root-mean squared deviation
        (RMSD). The sign of the RMSD is forced to be negative so as to allow
        for maximization of this objective function.

        Parameters
        ----------
        params : Sequence
            One or more model parameters
        observed : Sequence
            The observed values
        x : Sequence or None
            Input driver data

        Returns
        -------
        Number
            The (negative) root-mean squared deviation (RMSD) between the
            predicted and observed values
        &#39;&#39;&#39;
        predicted = self.model(params, *x)
        if self.weights is not None:
            return -np.sqrt(
                np.nanmean(((predicted - observed) * self.weights) ** 2))
        return -np.sqrt(np.nanmean(((predicted - observed)) ** 2))

    def loglik_gaussian(
            self, params: Sequence, observed: Sequence,
            x: Sequence = None) -&gt; Number:
        &#39;&#39;&#39;
        Gaussian log-likelihood, assuming independent, identically distributed
        observations.

        Parameters
        ----------
        params : Sequence
            One or more model parameters
        observed : Sequence
            The observed values
        x : Sequence or None
            Input driver data

        Returns
        -------
        Number
            The (negative) log-likelihood
        &#39;&#39;&#39;
        predicted = self.model(params, *x)
        sigma = params[-1]
        # Gaussian log-likelihood;
        # -\frac{N}{2}\,\mathrm{log}(2\pi\hat{\sigma}^2)
        #   - \frac{1}{2\hat{\sigma}^2} \sum (\hat{y} - y)^2
        return -0.5 * np.log(2 * np.pi * sigma**2) - (0.5 / sigma**2) *\
            np.nansum((predicted - observed)**2)

    def loglik_kge(
            self, params: Sequence, observed: Sequence,
            x: Sequence = None) -&gt; Number:
        r&#39;&#39;&#39;
        Kling-Gupta efficiency.

        $$
        KGE = 1 - \sqrt{(r - 1)^2 + (\alpha - 1)^2 + (\beta - 1)^2}
        $$

        Parameters
        ----------
        params : Sequence
            One or more model parameters
        observed : Sequence
            The observed values
        x : Sequence or None
            Input driver data

        Returns
        -------
        Number
            The Kling-Gupta efficiency
        &#39;&#39;&#39;
        predicted = self.model(params, *x)
        r = np.corrcoef(predicted, observed)[0, 1]
        alpha = np.std(predicted) / np.std(observed)
        beta = np.sum(predicted) / np.sum(observed)
        return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

    def perform(self, node, inputs, outputs):
        &#39;&#39;&#39;
        The method that is used when calling the Op.

        Parameters
        ----------
        node
        inputs : Sequence
        outputs : Sequence
        &#39;&#39;&#39;
        (params,) = inputs
        logl = self._loglik(params, self.observed, self.x)
        outputs[0][0] = np.array(logl) # Output the log-likelihood</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>aesara.graph.op.Op</li>
<li>aesara.graph.utils.MetaObject</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="mod17.calibration.BlackBoxLikelihood.default_output"><code class="name">var <span class="ident">default_output</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mod17.calibration.BlackBoxLikelihood.destroy_map"><code class="name">var <span class="ident">destroy_map</span> : Dict[int, List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mod17.calibration.BlackBoxLikelihood.itypes"><code class="name">var <span class="ident">itypes</span> : Optional[Sequence[Type]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mod17.calibration.BlackBoxLikelihood.otypes"><code class="name">var <span class="ident">otypes</span> : Optional[Sequence[Type]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mod17.calibration.BlackBoxLikelihood.params_type"><code class="name">var <span class="ident">params_type</span> : Optional[aesara.link.c.params_type.ParamsType]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mod17.calibration.BlackBoxLikelihood.view_map"><code class="name">var <span class="ident">view_map</span> : Dict[int, List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mod17.calibration.BlackBoxLikelihood.loglik"><code class="name flex">
<span>def <span class="ident">loglik</span></span>(<span>self, params: Sequence[+T_co], observed: Sequence[+T_co], x: Sequence[+T_co] = None) ‑> numbers.Number</span>
</code></dt>
<dd>
<div class="desc"><p>Pseudo-log likelihood, based on the root-mean squared deviation
(RMSD). The sign of the RMSD is forced to be negative so as to allow
for maximization of this objective function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>One or more model parameters</dd>
<dt><strong><code>observed</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>The observed values</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>Sequence</code> or <code>None</code></dt>
<dd>Input driver data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Number</code></dt>
<dd>The (negative) root-mean squared deviation (RMSD) between the
predicted and observed values</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def loglik(
        self, params: Sequence, observed: Sequence,
        x: Sequence = None) -&gt; Number:
    &#39;&#39;&#39;
    Pseudo-log likelihood, based on the root-mean squared deviation
    (RMSD). The sign of the RMSD is forced to be negative so as to allow
    for maximization of this objective function.

    Parameters
    ----------
    params : Sequence
        One or more model parameters
    observed : Sequence
        The observed values
    x : Sequence or None
        Input driver data

    Returns
    -------
    Number
        The (negative) root-mean squared deviation (RMSD) between the
        predicted and observed values
    &#39;&#39;&#39;
    predicted = self.model(params, *x)
    if self.weights is not None:
        return -np.sqrt(
            np.nanmean(((predicted - observed) * self.weights) ** 2))
    return -np.sqrt(np.nanmean(((predicted - observed)) ** 2))</code></pre>
</details>
</dd>
<dt id="mod17.calibration.BlackBoxLikelihood.loglik_gaussian"><code class="name flex">
<span>def <span class="ident">loglik_gaussian</span></span>(<span>self, params: Sequence[+T_co], observed: Sequence[+T_co], x: Sequence[+T_co] = None) ‑> numbers.Number</span>
</code></dt>
<dd>
<div class="desc"><p>Gaussian log-likelihood, assuming independent, identically distributed
observations.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>One or more model parameters</dd>
<dt><strong><code>observed</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>The observed values</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>Sequence</code> or <code>None</code></dt>
<dd>Input driver data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Number</code></dt>
<dd>The (negative) log-likelihood</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def loglik_gaussian(
        self, params: Sequence, observed: Sequence,
        x: Sequence = None) -&gt; Number:
    &#39;&#39;&#39;
    Gaussian log-likelihood, assuming independent, identically distributed
    observations.

    Parameters
    ----------
    params : Sequence
        One or more model parameters
    observed : Sequence
        The observed values
    x : Sequence or None
        Input driver data

    Returns
    -------
    Number
        The (negative) log-likelihood
    &#39;&#39;&#39;
    predicted = self.model(params, *x)
    sigma = params[-1]
    # Gaussian log-likelihood;
    # -\frac{N}{2}\,\mathrm{log}(2\pi\hat{\sigma}^2)
    #   - \frac{1}{2\hat{\sigma}^2} \sum (\hat{y} - y)^2
    return -0.5 * np.log(2 * np.pi * sigma**2) - (0.5 / sigma**2) *\
        np.nansum((predicted - observed)**2)</code></pre>
</details>
</dd>
<dt id="mod17.calibration.BlackBoxLikelihood.loglik_kge"><code class="name flex">
<span>def <span class="ident">loglik_kge</span></span>(<span>self, params: Sequence[+T_co], observed: Sequence[+T_co], x: Sequence[+T_co] = None) ‑> numbers.Number</span>
</code></dt>
<dd>
<div class="desc"><p>Kling-Gupta efficiency.</p>
<p><span><span class="MathJax_Preview">
KGE = 1 - \sqrt{(r - 1)^2 + (\alpha - 1)^2 + (\beta - 1)^2}
</span><script type="math/tex; mode=display">
KGE = 1 - \sqrt{(r - 1)^2 + (\alpha - 1)^2 + (\beta - 1)^2}
</script></span></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>One or more model parameters</dd>
<dt><strong><code>observed</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>The observed values</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>Sequence</code> or <code>None</code></dt>
<dd>Input driver data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Number</code></dt>
<dd>The Kling-Gupta efficiency</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def loglik_kge(
        self, params: Sequence, observed: Sequence,
        x: Sequence = None) -&gt; Number:
    r&#39;&#39;&#39;
    Kling-Gupta efficiency.

    $$
    KGE = 1 - \sqrt{(r - 1)^2 + (\alpha - 1)^2 + (\beta - 1)^2}
    $$

    Parameters
    ----------
    params : Sequence
        One or more model parameters
    observed : Sequence
        The observed values
    x : Sequence or None
        Input driver data

    Returns
    -------
    Number
        The Kling-Gupta efficiency
    &#39;&#39;&#39;
    predicted = self.model(params, *x)
    r = np.corrcoef(predicted, observed)[0, 1]
    alpha = np.std(predicted) / np.std(observed)
    beta = np.sum(predicted) / np.sum(observed)
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)</code></pre>
</details>
</dd>
<dt id="mod17.calibration.BlackBoxLikelihood.perform"><code class="name flex">
<span>def <span class="ident">perform</span></span>(<span>self, node, inputs, outputs)</span>
</code></dt>
<dd>
<div class="desc"><p>The method that is used when calling the Op.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>node</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>inputs</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>outputs</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def perform(self, node, inputs, outputs):
    &#39;&#39;&#39;
    The method that is used when calling the Op.

    Parameters
    ----------
    node
    inputs : Sequence
    outputs : Sequence
    &#39;&#39;&#39;
    (params,) = inputs
    logl = self._loglik(params, self.observed, self.x)
    outputs[0][0] = np.array(logl) # Output the log-likelihood</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="mod17.calibration.CalibrationAPI"><code class="flex name class">
<span>class <span class="ident">CalibrationAPI</span></span>
<span>(</span><span>config=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Convenience class for calibrating the MOD17 GPP and NPP models. Meant to
be used with <code>fire.Fire()</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CalibrationAPI(object):
    &#39;&#39;&#39;
    Convenience class for calibrating the MOD17 GPP and NPP models. Meant to
    be used with `fire.Fire()`.
    &#39;&#39;&#39;
    def __init__(self, config = None):
        config_file = config
        if config_file is None:
            config_file = os.path.join(
                MOD17_DIR, &#39;data/MOD17_calibration_config.json&#39;)
        with open(config_file, &#39;r&#39;) as file:
            self.config = json.load(file)
        self.hdf5 = self.config[&#39;data&#39;][&#39;file&#39;]

    def _clean(self, raw: Sequence, drivers: Sequence, protocol: str = &#39;GPP&#39;):
        &#39;Cleans up data values according to a prescribed protocol&#39;
        if protocol == &#39;GPP&#39;:
            # Filter out observed GPP values when GPP is negative or when
            #   APAR &lt; 0.1 g C m-2 day-1
            apar = drivers[&#39;fPAR&#39;] * drivers[&#39;PAR&#39;]
            return np.where(
                apar &lt; 0.1, np.nan, np.where(raw &lt; 0, np.nan, raw))

    def _filter(self, raw: Sequence, size: int):
        &#39;Apply a smoothing filter with zero phase offset&#39;
        if size &gt; 1:
            window = np.ones(size) / size
            return np.apply_along_axis(
                lambda x: signal.filtfilt(window, np.ones(1), x), 0, raw)
        return raw # Or, revert to the raw data

    def clean_observed(
            self, raw: Sequence, drivers: Sequence, driver_labels: Sequence,
            protocol: str = &#39;GPP&#39;, filter_length: int = 2) -&gt; Sequence:
        &#39;&#39;&#39;
        Cleans observed tower flux data according to a prescribed protocol.

        - For GPP data: Removes observations where GPP &lt; 0 or where APAR is
            &lt; 0.1 MJ m-2 day-1

        Parameters
        ----------
        raw : Sequence
        drivers : Sequence
        driver_labels : Sequence
        protocol : str
        filter_length : int
            The window size for the smoothing filter, applied to the observed
            data

        Returns
        -------
        Sequence
        &#39;&#39;&#39;
        if protocol != &#39;GPP&#39;:
            raise NotImplementedError(&#39;&#34;protocol&#34; must be one of: &#34;GPP&#34;&#39;)
        # Read in the observed data and apply smoothing filter
        obs = self._filter(raw, filter_length)
        obs = self._clean(obs, dict([
            (driver_labels[i], data)
            for i, data in enumerate(drivers)
        ]), protocol = &#39;GPP&#39;)
        return obs

    def export_bplut(
            self, model: str, output_path: str, thin: int = 10,
            burn: int = 1000):
        &#39;&#39;&#39;
        Export the BPLUT using the posterior mean from the MCMC sampler. NOTE:
        The posterior mean is usually not the best estimate for poorly
        identified parameters.

        Parameters
        ----------
        model : str
            The name of the model (&#34;GPP&#34; or &#34;NPP&#34;)
        output_path : str
            The output CSV file path
        thin : int
            Thinning rate
        burn : int
            The burn-in (i.e., first N samples to discard)
        &#39;&#39;&#39;
        params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][model])
        bplut = params_dict.copy()
        # Filter the parameters to just those for the PFT of interest
        for pft in PFT_VALID:
            backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] %\
                (model, pft)
            params = dict([(k, v[pft]) for k, v in params_dict.items()])
            sampler = MOD17StochasticSampler(
                self.config, getattr(MOD17, &#39;_%s&#39; % model.lower()), params,
                backend = backend)
            trace = sampler.get_trace()
            fit = trace.sel(
                draw = slice(burn, None, thin))[&#39;posterior&#39;].mean()
            for each in MOD17.required_parameters:
                try:
                    bplut[each][pft] = float(fit[each])
                except KeyError:
                    continue
        write_bplut(bplut, output_path)

    def export_posterior(
            self, model: str, param: str, output_path: str, thin: int = 10,
            burn: int = 1000, k_folds: int = 1):
        &#39;&#39;&#39;
        Exports posterior distribution for a parameter, for each PFT to HDF5.

        Parameters
        ----------
        model : str
            The name of the model (&#34;GPP&#34; or &#34;NPP&#34;)
        param : str
            The model parameter to export
        output_path : str
            The output HDF5 file path
        thin : int
            Thinning rate
        burn : int
            The burn-in (i.e., first N samples to discard)
        k_folds : int
            The number of k-folds used in cross-calibration/validation;
            if more than one (default), the folds for each PFT will be
            combined into a single HDF5 file
        &#39;&#39;&#39;
        params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][model])
        bplut = params_dict.copy()
        # Filter the parameters to just those for the PFT of interest
        post = []
        for pft in PFT_VALID:
            params = dict([(k, v[pft]) for k, v in params_dict.items()])
            backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] %\
                (model, pft)
            post_by_fold = []
            for fold in range(1, k_folds + 1):
                if k_folds &gt; 1:
                    backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] %\
                        (f&#39;{model}-k{fold}&#39;, pft)
                sampler = MOD17StochasticSampler(
                    self.config, getattr(MOD17, &#39;_%s&#39; % model.lower()), params,
                    backend = backend)
                trace = sampler.get_trace()
                fit = trace.sel(draw = slice(burn, None, thin))[&#39;posterior&#39;]
                num_samples = fit.sizes[&#39;chain&#39;] * fit.sizes[&#39;draw&#39;]
                if param in fit:
                    post_by_fold.append(
                        az.extract_dataset(fit, combined = True)[param].values)
                else:
                    # In case there is, e.g., a parameter that takes on a
                    #   constant value for a specific PFT
                    if k_folds &gt; 1:
                        post_by_fold.append(np.ones((1, num_samples)) * np.nan)
                    else:
                        post_by_fold.append(np.ones(num_samples) * np.nan)
            if k_folds &gt; 1:
                post.append(np.vstack(post_by_fold))
            else:
                post.extend(post_by_fold)
        # If not every PFT&#39;s posterior has the same number of samples (e.g.,
        #   when one set of chains was run longer than another)...
        if not all([p.shape == post[0].shape for p in post]):
            max_len = max([p.shape for p in post])[0]
            # ...Reshape all posteriors to match the greatest sample size
            import ipdb
            ipdb.set_trace()#FIXME
            post = [
                np.pad(
                    p.astype(np.float32), (0, max_len - p.size),
                    mode = &#39;constant&#39;, constant_values = (np.nan,))
                for p in post
            ]
        with h5py.File(output_path, &#39;a&#39;) as hdf:
            post = np.stack(post)
            ts = datetime.date.today().strftime(&#39;%Y-%m-%d&#39;) # Today&#39;s date
            dataset = hdf.create_dataset(
                f&#39;{param}_posterior&#39;, post.shape, np.float32, post)
            dataset.attrs[&#39;description&#39;] = &#39;CalibrationAPI.export_posterior() on {ts}&#39;

    def export_likely_posterior(
            self, model: str, param: str, output_path: str, thin: int = 10,
            burn: int = 1000, ptile: int = 95):
        &#39;&#39;&#39;
        Exports posterior distribution for a parameter based on likelihood

        Parameters
        ----------
        model : str
            The name of the model (&#34;GPP&#34; or &#34;NPP&#34;)
        param : str
            The model parameter to export
        output_path : str
            The output HDF5 file path
        thin : int
            Thinning rate
        burn : int
            The burn-in (i.e., first N samples to discard)
        ptile : int
            The percentile cutoff for likelihood; only samples at or above
            this likelihood cutoff will be included
        &#39;&#39;&#39;
        params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][model])
        bplut = params_dict.copy()
        # Filter the parameters to just those for the PFT of interest
        post = []
        likelihood = []
        for pft in PFT_VALID:
            backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] % (model, pft)
            params = dict([(k, v[pft]) for k, v in params_dict.items()])
            sampler = MOD17StochasticSampler(
                self.config, getattr(MOD17, &#39;_%s&#39; % model.lower()), params,
                backend = backend)
            trace = sampler.get_trace()
            fit = trace.sel(draw = slice(burn, None, thin))
            # Find the likelihood value associated with the cutoff percentile
            ll = az.extract_dataset(
                fit, combined = True)[&#39;log_likelihood&#39;].values
            values = az.extract_dataset(fit, combined = True)[param].values
            cutoff = np.percentile(ll, ptile)
            post.append(values[ll &gt;= cutoff])
            likelihood.append(ll[ll &gt;= cutoff])
        with h5py.File(output_path, &#39;a&#39;) as hdf:
            n = max([len(p) for p in post])
            # Make sure all arrays are the same size
            post = np.stack([
                np.concatenate((p, np.full((n - len(p),), np.nan)))
                for p in post
            ])
            likelihood = np.stack([
                np.concatenate((p, np.full((n - len(p),), np.nan)))
                for p in likelihood
            ])
            ts = datetime.date.today().strftime(&#39;%Y-%m-%d&#39;) # Today&#39;s date
            dataset = hdf.create_dataset(
                f&#39;{param}_posterior&#39;, post.shape, np.float32, post)
            dataset.attrs[&#39;description&#39;] = &#39;CalibrationAPI.export_likely_posterior() on {ts}&#39;
            dataset = hdf.create_dataset(
                f&#39;{param}_likelihood&#39;, likelihood.shape, np.float32, likelihood)

    def tune_gpp(
            self, pft: int, plot_trace: bool = False, ipdb: bool = False,
            save_fig: bool = False, **kwargs):
        &#39;&#39;&#39;
        Run the MOD17 GPP calibration.

        Parameters
        ----------
        pft : int
            The Plant Functional Type (PFT) to calibrate
        plot_trace : bool
            True to plot the trace for a previous calibration run; this will
            also NOT start a new calibration (Default: False)
        ipdb : bool
            True to drop the user into an ipdb prompt, prior to and instead of
            running calibration
        save_fig : bool
            True to save figures to files instead of showing them
            (Default: False)
        **kwargs
            Additional keyword arguments passed to
            `MOD17StochasticSampler.run()`
        &#39;&#39;&#39;
        assert pft in PFT_VALID, f&#39;Invalid PFT: {pft}&#39;
        # Pass configuration parameters to MOD17StochasticSampler.run()
        for key in (&#39;chains&#39;, &#39;draws&#39;, &#39;tune&#39;, &#39;scaling&#39;):
            if key in self.config[&#39;optimization&#39;].keys() and not key in kwargs.keys():
                kwargs[key] = self.config[&#39;optimization&#39;][key]
        # Filter the parameters to just those for the PFT of interest
        params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][&#39;GPP&#39;])
        # Load blacklisted sites (if any)
        blacklist = self.config[&#39;data&#39;][&#39;sites_blacklisted&#39;]
        params_dict = dict([(k, v[pft]) for k, v in params_dict.items()])
        model = MOD17(params_dict)
        objective = self.config[&#39;optimization&#39;][&#39;objective&#39;].lower()

        print(&#39;Loading driver datasets...&#39;)
        with h5py.File(self.hdf5, &#39;r&#39;) as hdf:
            sites = hdf[&#39;FLUXNET/site_id&#39;][:]
            if hasattr(sites[0], &#39;decode&#39;):
                sites = list(map(lambda x: x.decode(&#39;utf-8&#39;), sites))
            # Get dominant PFT
            pft_map = pft_dominant(hdf[&#39;state/PFT&#39;][:], site_list = sites)
            # Blacklist various sites
            pft_mask = np.logical_and(pft_map == pft, ~np.in1d(sites, blacklist))
            # NOTE: Converting from Kelvin to Celsius
            tday = hdf[&#39;MERRA2/T10M_daytime&#39;][:][:,pft_mask] - 273.15
            qv10m = hdf[&#39;MERRA2/QV10M_daytime&#39;][:][:,pft_mask]
            ps = hdf[&#39;MERRA2/PS_daytime&#39;][:][:,pft_mask]
            drivers = [ # fPAR, Tmin, VPD, PAR
                hdf[&#39;MODIS/MOD15A2HGF_fPAR_interp&#39;][:][:,pft_mask],
                hdf[&#39;MERRA2/Tmin&#39;][:][:,pft_mask] - 273.15,
                MOD17.vpd(qv10m, ps, tday),
                MOD17.par(hdf[&#39;MERRA2/SWGDN&#39;][:][:,pft_mask]),
            ]
            # Convert fPAR from (%) to [0,1]
            drivers[0] = np.nanmean(drivers[0], axis = -1) / 100
            # If RMSE is used, then we want to pay attention to weighting
            weights = None
            if objective in (&#39;rmsd&#39;, &#39;rmse&#39;):
                weights = hdf[&#39;weights&#39;][pft_mask][np.newaxis,:]\
                    .repeat(tday.shape[0], axis = 0)
            # Check that driver data do not contain NaNs
            for d, each in enumerate(drivers):
                name = (&#39;fPAR&#39;, &#39;Tmin&#39;, &#39;VPD&#39;, &#39;PAR&#39;)[d]
                assert not np.isnan(each).any(),\
                    f&#39;Driver dataset &#34;{name}&#34; contains NaNs&#39;
            tower_gpp = hdf[&#39;FLUXNET/GPP&#39;][:][:,pft_mask]
            # Read the validation mask; mask out observations that are
            #   reserved for validation
            print(&#39;Masking out validation data...&#39;)
            mask = hdf[&#39;FLUXNET/validation_mask&#39;][pft]
            tower_gpp[mask] = np.nan

        # Clean observations, then mask out driver data where the are no
        #   observations
        tower_gpp = self.clean_observed(
            tower_gpp, drivers, MOD17StochasticSampler.required_drivers[&#39;GPP&#39;],
            protocol = &#39;GPP&#39;)
        if weights is not None:
            weights = weights[~np.isnan(tower_gpp)]
        for d, _ in enumerate(drivers):
            drivers[d] = drivers[d][~np.isnan(tower_gpp)]
        tower_gpp = tower_gpp[~np.isnan(tower_gpp)]

        print(&#39;Initializing sampler...&#39;)
        backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] % (&#39;GPP&#39;, pft)
        sampler = MOD17StochasticSampler(
            self.config, MOD17._gpp, params_dict, backend = backend,
            weights = weights)
        if plot_trace or ipdb:
            if ipdb:
                import ipdb
                ipdb.set_trace()
            trace = sampler.get_trace()
            az.plot_trace(trace, var_names = MOD17.required_parameters[0:5])
            pyplot.show()
            return
        # Get (informative) priors for just those parameters that have them
        with open(self.config[&#39;optimization&#39;][&#39;prior&#39;], &#39;r&#39;) as file:
            prior = json.load(file)
        prior_params = filter(
            lambda p: p in prior.keys(), sampler.required_parameters[&#39;GPP&#39;])
        prior = dict([
            (p, {&#39;mu&#39;: prior[p][&#39;mu&#39;][pft], &#39;sigma&#39;: prior[p][&#39;sigma&#39;][pft]})
            for p in prior_params
        ])
        sampler.run(
            tower_gpp, drivers, prior = prior, save_fig = save_fig, **kwargs)

    def tune_npp(
            self, pft: int, plot_trace: bool = False, ipdb: bool = False,
            save_fig: bool = False, climatology = False,
            cutoff: Number = 2385, k_folds: int = 1, **kwargs):
        r&#39;&#39;&#39;
        Run the MOD17 NPP calibration. If k-folds cross-validation is used,
        the model is calibrated on $k$ random subsets of the data and a
        series of file is created, e.g., as:

            MOD17_NPP_calibration_PFT1.h5
            MOD17_NPP-k1_calibration_PFT1.nc4
            MOD17_NPP-k2_calibration_PFT1.nc4
            ...

        Where each `.nc4` file is a standard `arviz` backend and the `.h5`
        indicates which indices from the NPP observations vector, after
        removing NaNs, were excluded (i.e., the indices of the test data).

        Parameters
        ----------
        pft : int
            The Plant Functional Type (PFT) to calibrate
        plot_trace : bool
            True to display the trace plot ONLY and not run calibration
            (Default: False)
        ipdb : bool
            True to drop into an interactive Python debugger (`ipdb`) after
            loading an existing trace (Default: False)
        save_fig : bool
            True to save the post-calibration trace plot to a file instead of
            displaying it (Default: False)
        climatology : bool
            True to use a MERRA-2 climatology (and look for it in the drivers
            file), i.e., use `MERRA2_climatology` group instead of
            `surface_met_MERRA2` group (Default: False)
        cutoff : Number
            Maximum value of observed NPP (g C m-2 year-1); values above this
            cutoff will be discarded and not used in calibration
            (Default: 2385)
        k_folds : int
            Number of folds to use in k-folds cross-validation; defaults to
            k=1, i.e., no cross-validation is performed.
        **kwargs
            Additional keyword arguments passed to
            `MOD17StochasticSampler.run()`
        &#39;&#39;&#39;
        assert pft in PFT_VALID, f&#39;Invalid PFT: {pft}&#39;
        prefix = &#39;MERRA2_climatology&#39; if climatology else &#39;surface_met_MERRA2&#39;
        params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][&#39;NPP&#39;])
        # Filter the parameters to just those for the PFT of interest
        params_dict = dict([(k, v[pft]) for k, v in params_dict.items()])
        model = MOD17(params_dict)
        kwargs.update({&#39;var_names&#39;: [
            &#39;~LUE_max&#39;, &#39;~tmin0&#39;, &#39;~tmin1&#39;, &#39;~vpd0&#39;, &#39;~vpd1&#39;, &#39;~log_likelihood&#39;
        ]})
        # Pass configuration parameters to MOD17StochasticSampler.run()
        for key in (&#39;chains&#39;, &#39;draws&#39;, &#39;tune&#39;, &#39;scaling&#39;):
            if key in self.config[&#39;optimization&#39;].keys():
                kwargs[key] = self.config[&#39;optimization&#39;][key]
        print(&#39;Loading driver datasets...&#39;)
        with h5py.File(self.hdf5, &#39;r&#39;) as hdf:
            # NOTE: This is only recorded at the site-level; no need to
            #   determine modal PFT across subgrid
            pft_map = hdf[&#39;NPP/PFT&#39;][:]
            # Leave out sites where there is no fPAR (and no LAI) data
            fpar = hdf[&#39;NPP/MOD15A2H_fPAR_clim_filt&#39;][:]
            mask = np.logical_and(
                    pft_map == pft, ~np.isnan(np.nanmean(fpar, axis = -1))\
                .all(axis = 0))
            # NOTE: Converting from Kelvin to Celsius
            tday = hdf[f&#39;NPP/{prefix}/T10M_daytime&#39;][:][:,mask] - 273.15
            qv10m = hdf[f&#39;NPP/{prefix}/QV10M_daytime&#39;][:][:,mask]
            ps = hdf[f&#39;NPP/{prefix}/PS_daytime&#39;][:][:,mask]
            drivers = [ # fPAR, Tmin, VPD, PAR, LAI, Tmean, years
                hdf[&#39;NPP/MOD15A2H_fPAR_clim_filt&#39;][:][:,mask],
                hdf[f&#39;NPP/{prefix}/Tmin&#39;][:][:,mask]  - 273.15,
                MOD17.vpd(qv10m, ps, tday),
                MOD17.par(hdf[f&#39;NPP/{prefix}/SWGDN&#39;][:][:,mask]),
                hdf[&#39;NPP/MOD15A2H_LAI_clim_filt&#39;][:][:,mask],
                hdf[f&#39;NPP/{prefix}/T10M&#39;][:][:,mask] - 273.15,
                np.full(ps.shape, 1) # i.e., A 365-day climatological year (&#34;Year 1&#34;)
            ]
            observed_npp = hdf[&#39;NPP/NPP_total&#39;][:][mask]
        if cutoff is not None:
            observed_npp[observed_npp &gt; cutoff] = np.nan
        # Set negative VPD to zero
        drivers[2] = np.where(drivers[2] &lt; 0, 0, drivers[2])
        # Convert fPAR from (%) to [0,1] and re-scale LAI; reshape fPAR, LAI
        drivers[0] = np.nanmean(drivers[0], axis = -1) * 0.01
        drivers[4] = np.nanmean(drivers[4], axis = -1) * 0.1
        # TODO Mask out driver data where the are no observations
        for d, _ in enumerate(drivers):
            drivers[d] = drivers[d][:,~np.isnan(observed_npp)]
        observed_npp = observed_npp[~np.isnan(observed_npp)]

        if k_folds &gt; 1:
            # Back-up the original (complete) datasets
            _drivers = [d.copy() for d in drivers]
            _observed_npp = observed_npp.copy()
            # Randomize the indices of the NPP data
            indices = np.arange(0, observed_npp.size)
            np.random.shuffle(indices)
            # Get the starting and ending index of each fold
            fold_idx = np.array([indices.size // k_folds] * k_folds) * np.arange(0, k_folds)
            fold_idx = list(map(list, zip(fold_idx, fold_idx + indices.size // k_folds)))
            # Ensure that the entire dataset is used
            fold_idx[-1][-1] = indices.max()
            idx_test = [indices[start:end] for start, end in fold_idx]

        # Loop over each fold (or the entire dataset, if num. folds == 1)
        for k, fold in enumerate(range(1, k_folds + 1)):
            backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] % (&#39;NPP&#39;, pft)
            if k_folds &gt; 1 and fold == 1:
                # Create an HDF5 file with the same name as the (original)
                #   netCDF4 back-end, store the test indices
                with h5py.File(backend.replace(&#39;nc4&#39;, &#39;h5&#39;), &#39;w&#39;) as hdf:
                    out = list(idx_test)
                    size = indices.size // k_folds
                    try:
                        out = np.stack(out)
                    except ValueError:
                        size = max((o.size for o in out))
                        for i in range(0, len(out)):
                            out[i] = np.concatenate((out[i], [np.nan] * (size - out[i].size)))
                    hdf.create_dataset(
                        &#39;test_indices&#39;, (k_folds, size), np.int32, np.stack(out))
                # Restore the original NPP dataset
                observed_npp = _observed_npp.copy()
                # Set to NaN all the test indices
                idx = idx_test[k]
                observed_npp[idx] = np.nan
                # Same for drivers, after restoring from the original
                drivers = [d.copy()[:,~np.isnan(observed_npp)] for d in _drivers]
                observed_npp = observed_npp[~np.isnan(observed_npp)]
            # Use a different naming scheme for the backend
            if k_folds &gt; 1:
                backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] % (f&#39;NPP-k{fold}&#39;, pft)

            print(&#39;Initializing sampler...&#39;)
            sampler = MOD17StochasticSampler(
                self.config, MOD17._npp, params_dict, backend = backend,
                model_name = &#39;NPP&#39;)
            if plot_trace or ipdb:
                if ipdb:
                    import ipdb
                    ipdb.set_trace()
                trace = sampler.get_trace()
                az.plot_trace(
                    trace, var_names = MOD17.required_parameters[5:])
                pyplot.show()
                return
            # Get (informative) priors for just those parameters that have them
            with open(self.config[&#39;optimization&#39;][&#39;prior&#39;], &#39;r&#39;) as file:
                prior = json.load(file)
            prior_params = filter(
                lambda p: p in prior.keys(), sampler.required_parameters[&#39;NPP&#39;])
            prior = dict([
                (p, prior[p]) for p in prior_params
            ])
            for key in prior.keys():
                # And make sure to subset to the chosen PFT!
                for arg in prior[key].keys():
                    prior[key][arg] = prior[key][arg][pft]
            sampler.run(
                observed_npp, drivers, prior = prior, save_fig = save_fig,
                **kwargs)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mod17.viirs.VIIRSCalibrationAPI" href="viirs.html#mod17.viirs.VIIRSCalibrationAPI">VIIRSCalibrationAPI</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mod17.calibration.CalibrationAPI.clean_observed"><code class="name flex">
<span>def <span class="ident">clean_observed</span></span>(<span>self, raw: Sequence[+T_co], drivers: Sequence[+T_co], driver_labels: Sequence[+T_co], protocol: str = 'GPP', filter_length: int = 2) ‑> Sequence[+T_co]</span>
</code></dt>
<dd>
<div class="desc"><p>Cleans observed tower flux data according to a prescribed protocol.</p>
<ul>
<li>For GPP data: Removes observations where GPP &lt; 0 or where APAR is
&lt; 0.1 MJ m-2 day-1</li>
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>raw</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>drivers</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>driver_labels</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>protocol</code></strong> :&ensp;<code>str</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>filter_length</code></strong> :&ensp;<code>int</code></dt>
<dd>The window size for the smoothing filter, applied to the observed
data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Sequence</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clean_observed(
        self, raw: Sequence, drivers: Sequence, driver_labels: Sequence,
        protocol: str = &#39;GPP&#39;, filter_length: int = 2) -&gt; Sequence:
    &#39;&#39;&#39;
    Cleans observed tower flux data according to a prescribed protocol.

    - For GPP data: Removes observations where GPP &lt; 0 or where APAR is
        &lt; 0.1 MJ m-2 day-1

    Parameters
    ----------
    raw : Sequence
    drivers : Sequence
    driver_labels : Sequence
    protocol : str
    filter_length : int
        The window size for the smoothing filter, applied to the observed
        data

    Returns
    -------
    Sequence
    &#39;&#39;&#39;
    if protocol != &#39;GPP&#39;:
        raise NotImplementedError(&#39;&#34;protocol&#34; must be one of: &#34;GPP&#34;&#39;)
    # Read in the observed data and apply smoothing filter
    obs = self._filter(raw, filter_length)
    obs = self._clean(obs, dict([
        (driver_labels[i], data)
        for i, data in enumerate(drivers)
    ]), protocol = &#39;GPP&#39;)
    return obs</code></pre>
</details>
</dd>
<dt id="mod17.calibration.CalibrationAPI.export_bplut"><code class="name flex">
<span>def <span class="ident">export_bplut</span></span>(<span>self, model: str, output_path: str, thin: int = 10, burn: int = 1000)</span>
</code></dt>
<dd>
<div class="desc"><p>Export the BPLUT using the posterior mean from the MCMC sampler. NOTE:
The posterior mean is usually not the best estimate for poorly
identified parameters.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the model ("GPP" or "NPP")</dd>
<dt><strong><code>output_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The output CSV file path</dd>
<dt><strong><code>thin</code></strong> :&ensp;<code>int</code></dt>
<dd>Thinning rate</dd>
<dt><strong><code>burn</code></strong> :&ensp;<code>int</code></dt>
<dd>The burn-in (i.e., first N samples to discard)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_bplut(
        self, model: str, output_path: str, thin: int = 10,
        burn: int = 1000):
    &#39;&#39;&#39;
    Export the BPLUT using the posterior mean from the MCMC sampler. NOTE:
    The posterior mean is usually not the best estimate for poorly
    identified parameters.

    Parameters
    ----------
    model : str
        The name of the model (&#34;GPP&#34; or &#34;NPP&#34;)
    output_path : str
        The output CSV file path
    thin : int
        Thinning rate
    burn : int
        The burn-in (i.e., first N samples to discard)
    &#39;&#39;&#39;
    params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][model])
    bplut = params_dict.copy()
    # Filter the parameters to just those for the PFT of interest
    for pft in PFT_VALID:
        backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] %\
            (model, pft)
        params = dict([(k, v[pft]) for k, v in params_dict.items()])
        sampler = MOD17StochasticSampler(
            self.config, getattr(MOD17, &#39;_%s&#39; % model.lower()), params,
            backend = backend)
        trace = sampler.get_trace()
        fit = trace.sel(
            draw = slice(burn, None, thin))[&#39;posterior&#39;].mean()
        for each in MOD17.required_parameters:
            try:
                bplut[each][pft] = float(fit[each])
            except KeyError:
                continue
    write_bplut(bplut, output_path)</code></pre>
</details>
</dd>
<dt id="mod17.calibration.CalibrationAPI.export_likely_posterior"><code class="name flex">
<span>def <span class="ident">export_likely_posterior</span></span>(<span>self, model: str, param: str, output_path: str, thin: int = 10, burn: int = 1000, ptile: int = 95)</span>
</code></dt>
<dd>
<div class="desc"><p>Exports posterior distribution for a parameter based on likelihood</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the model ("GPP" or "NPP")</dd>
<dt><strong><code>param</code></strong> :&ensp;<code>str</code></dt>
<dd>The model parameter to export</dd>
<dt><strong><code>output_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The output HDF5 file path</dd>
<dt><strong><code>thin</code></strong> :&ensp;<code>int</code></dt>
<dd>Thinning rate</dd>
<dt><strong><code>burn</code></strong> :&ensp;<code>int</code></dt>
<dd>The burn-in (i.e., first N samples to discard)</dd>
<dt><strong><code>ptile</code></strong> :&ensp;<code>int</code></dt>
<dd>The percentile cutoff for likelihood; only samples at or above
this likelihood cutoff will be included</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_likely_posterior(
        self, model: str, param: str, output_path: str, thin: int = 10,
        burn: int = 1000, ptile: int = 95):
    &#39;&#39;&#39;
    Exports posterior distribution for a parameter based on likelihood

    Parameters
    ----------
    model : str
        The name of the model (&#34;GPP&#34; or &#34;NPP&#34;)
    param : str
        The model parameter to export
    output_path : str
        The output HDF5 file path
    thin : int
        Thinning rate
    burn : int
        The burn-in (i.e., first N samples to discard)
    ptile : int
        The percentile cutoff for likelihood; only samples at or above
        this likelihood cutoff will be included
    &#39;&#39;&#39;
    params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][model])
    bplut = params_dict.copy()
    # Filter the parameters to just those for the PFT of interest
    post = []
    likelihood = []
    for pft in PFT_VALID:
        backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] % (model, pft)
        params = dict([(k, v[pft]) for k, v in params_dict.items()])
        sampler = MOD17StochasticSampler(
            self.config, getattr(MOD17, &#39;_%s&#39; % model.lower()), params,
            backend = backend)
        trace = sampler.get_trace()
        fit = trace.sel(draw = slice(burn, None, thin))
        # Find the likelihood value associated with the cutoff percentile
        ll = az.extract_dataset(
            fit, combined = True)[&#39;log_likelihood&#39;].values
        values = az.extract_dataset(fit, combined = True)[param].values
        cutoff = np.percentile(ll, ptile)
        post.append(values[ll &gt;= cutoff])
        likelihood.append(ll[ll &gt;= cutoff])
    with h5py.File(output_path, &#39;a&#39;) as hdf:
        n = max([len(p) for p in post])
        # Make sure all arrays are the same size
        post = np.stack([
            np.concatenate((p, np.full((n - len(p),), np.nan)))
            for p in post
        ])
        likelihood = np.stack([
            np.concatenate((p, np.full((n - len(p),), np.nan)))
            for p in likelihood
        ])
        ts = datetime.date.today().strftime(&#39;%Y-%m-%d&#39;) # Today&#39;s date
        dataset = hdf.create_dataset(
            f&#39;{param}_posterior&#39;, post.shape, np.float32, post)
        dataset.attrs[&#39;description&#39;] = &#39;CalibrationAPI.export_likely_posterior() on {ts}&#39;
        dataset = hdf.create_dataset(
            f&#39;{param}_likelihood&#39;, likelihood.shape, np.float32, likelihood)</code></pre>
</details>
</dd>
<dt id="mod17.calibration.CalibrationAPI.export_posterior"><code class="name flex">
<span>def <span class="ident">export_posterior</span></span>(<span>self, model: str, param: str, output_path: str, thin: int = 10, burn: int = 1000, k_folds: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Exports posterior distribution for a parameter, for each PFT to HDF5.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the model ("GPP" or "NPP")</dd>
<dt><strong><code>param</code></strong> :&ensp;<code>str</code></dt>
<dd>The model parameter to export</dd>
<dt><strong><code>output_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The output HDF5 file path</dd>
<dt><strong><code>thin</code></strong> :&ensp;<code>int</code></dt>
<dd>Thinning rate</dd>
<dt><strong><code>burn</code></strong> :&ensp;<code>int</code></dt>
<dd>The burn-in (i.e., first N samples to discard)</dd>
<dt><strong><code>k_folds</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of k-folds used in cross-calibration/validation;
if more than one (default), the folds for each PFT will be
combined into a single HDF5 file</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_posterior(
        self, model: str, param: str, output_path: str, thin: int = 10,
        burn: int = 1000, k_folds: int = 1):
    &#39;&#39;&#39;
    Exports posterior distribution for a parameter, for each PFT to HDF5.

    Parameters
    ----------
    model : str
        The name of the model (&#34;GPP&#34; or &#34;NPP&#34;)
    param : str
        The model parameter to export
    output_path : str
        The output HDF5 file path
    thin : int
        Thinning rate
    burn : int
        The burn-in (i.e., first N samples to discard)
    k_folds : int
        The number of k-folds used in cross-calibration/validation;
        if more than one (default), the folds for each PFT will be
        combined into a single HDF5 file
    &#39;&#39;&#39;
    params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][model])
    bplut = params_dict.copy()
    # Filter the parameters to just those for the PFT of interest
    post = []
    for pft in PFT_VALID:
        params = dict([(k, v[pft]) for k, v in params_dict.items()])
        backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] %\
            (model, pft)
        post_by_fold = []
        for fold in range(1, k_folds + 1):
            if k_folds &gt; 1:
                backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] %\
                    (f&#39;{model}-k{fold}&#39;, pft)
            sampler = MOD17StochasticSampler(
                self.config, getattr(MOD17, &#39;_%s&#39; % model.lower()), params,
                backend = backend)
            trace = sampler.get_trace()
            fit = trace.sel(draw = slice(burn, None, thin))[&#39;posterior&#39;]
            num_samples = fit.sizes[&#39;chain&#39;] * fit.sizes[&#39;draw&#39;]
            if param in fit:
                post_by_fold.append(
                    az.extract_dataset(fit, combined = True)[param].values)
            else:
                # In case there is, e.g., a parameter that takes on a
                #   constant value for a specific PFT
                if k_folds &gt; 1:
                    post_by_fold.append(np.ones((1, num_samples)) * np.nan)
                else:
                    post_by_fold.append(np.ones(num_samples) * np.nan)
        if k_folds &gt; 1:
            post.append(np.vstack(post_by_fold))
        else:
            post.extend(post_by_fold)
    # If not every PFT&#39;s posterior has the same number of samples (e.g.,
    #   when one set of chains was run longer than another)...
    if not all([p.shape == post[0].shape for p in post]):
        max_len = max([p.shape for p in post])[0]
        # ...Reshape all posteriors to match the greatest sample size
        import ipdb
        ipdb.set_trace()#FIXME
        post = [
            np.pad(
                p.astype(np.float32), (0, max_len - p.size),
                mode = &#39;constant&#39;, constant_values = (np.nan,))
            for p in post
        ]
    with h5py.File(output_path, &#39;a&#39;) as hdf:
        post = np.stack(post)
        ts = datetime.date.today().strftime(&#39;%Y-%m-%d&#39;) # Today&#39;s date
        dataset = hdf.create_dataset(
            f&#39;{param}_posterior&#39;, post.shape, np.float32, post)
        dataset.attrs[&#39;description&#39;] = &#39;CalibrationAPI.export_posterior() on {ts}&#39;</code></pre>
</details>
</dd>
<dt id="mod17.calibration.CalibrationAPI.tune_gpp"><code class="name flex">
<span>def <span class="ident">tune_gpp</span></span>(<span>self, pft: int, plot_trace: bool = False, ipdb: bool = False, save_fig: bool = False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Run the MOD17 GPP calibration.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>pft</code></strong> :&ensp;<code>int</code></dt>
<dd>The Plant Functional Type (PFT) to calibrate</dd>
<dt><strong><code>plot_trace</code></strong> :&ensp;<code>bool</code></dt>
<dd>True to plot the trace for a previous calibration run; this will
also NOT start a new calibration (Default: False)</dd>
<dt><strong><code>ipdb</code></strong> :&ensp;<code>bool</code></dt>
<dd>True to drop the user into an ipdb prompt, prior to and instead of
running calibration</dd>
<dt><strong><code>save_fig</code></strong> :&ensp;<code>bool</code></dt>
<dd>True to save figures to files instead of showing them
(Default: False)</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments passed to
<code><a title="mod17.calibration.MOD17StochasticSampler.run" href="#mod17.calibration.StochasticSampler.run">StochasticSampler.run()</a></code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tune_gpp(
        self, pft: int, plot_trace: bool = False, ipdb: bool = False,
        save_fig: bool = False, **kwargs):
    &#39;&#39;&#39;
    Run the MOD17 GPP calibration.

    Parameters
    ----------
    pft : int
        The Plant Functional Type (PFT) to calibrate
    plot_trace : bool
        True to plot the trace for a previous calibration run; this will
        also NOT start a new calibration (Default: False)
    ipdb : bool
        True to drop the user into an ipdb prompt, prior to and instead of
        running calibration
    save_fig : bool
        True to save figures to files instead of showing them
        (Default: False)
    **kwargs
        Additional keyword arguments passed to
        `MOD17StochasticSampler.run()`
    &#39;&#39;&#39;
    assert pft in PFT_VALID, f&#39;Invalid PFT: {pft}&#39;
    # Pass configuration parameters to MOD17StochasticSampler.run()
    for key in (&#39;chains&#39;, &#39;draws&#39;, &#39;tune&#39;, &#39;scaling&#39;):
        if key in self.config[&#39;optimization&#39;].keys() and not key in kwargs.keys():
            kwargs[key] = self.config[&#39;optimization&#39;][key]
    # Filter the parameters to just those for the PFT of interest
    params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][&#39;GPP&#39;])
    # Load blacklisted sites (if any)
    blacklist = self.config[&#39;data&#39;][&#39;sites_blacklisted&#39;]
    params_dict = dict([(k, v[pft]) for k, v in params_dict.items()])
    model = MOD17(params_dict)
    objective = self.config[&#39;optimization&#39;][&#39;objective&#39;].lower()

    print(&#39;Loading driver datasets...&#39;)
    with h5py.File(self.hdf5, &#39;r&#39;) as hdf:
        sites = hdf[&#39;FLUXNET/site_id&#39;][:]
        if hasattr(sites[0], &#39;decode&#39;):
            sites = list(map(lambda x: x.decode(&#39;utf-8&#39;), sites))
        # Get dominant PFT
        pft_map = pft_dominant(hdf[&#39;state/PFT&#39;][:], site_list = sites)
        # Blacklist various sites
        pft_mask = np.logical_and(pft_map == pft, ~np.in1d(sites, blacklist))
        # NOTE: Converting from Kelvin to Celsius
        tday = hdf[&#39;MERRA2/T10M_daytime&#39;][:][:,pft_mask] - 273.15
        qv10m = hdf[&#39;MERRA2/QV10M_daytime&#39;][:][:,pft_mask]
        ps = hdf[&#39;MERRA2/PS_daytime&#39;][:][:,pft_mask]
        drivers = [ # fPAR, Tmin, VPD, PAR
            hdf[&#39;MODIS/MOD15A2HGF_fPAR_interp&#39;][:][:,pft_mask],
            hdf[&#39;MERRA2/Tmin&#39;][:][:,pft_mask] - 273.15,
            MOD17.vpd(qv10m, ps, tday),
            MOD17.par(hdf[&#39;MERRA2/SWGDN&#39;][:][:,pft_mask]),
        ]
        # Convert fPAR from (%) to [0,1]
        drivers[0] = np.nanmean(drivers[0], axis = -1) / 100
        # If RMSE is used, then we want to pay attention to weighting
        weights = None
        if objective in (&#39;rmsd&#39;, &#39;rmse&#39;):
            weights = hdf[&#39;weights&#39;][pft_mask][np.newaxis,:]\
                .repeat(tday.shape[0], axis = 0)
        # Check that driver data do not contain NaNs
        for d, each in enumerate(drivers):
            name = (&#39;fPAR&#39;, &#39;Tmin&#39;, &#39;VPD&#39;, &#39;PAR&#39;)[d]
            assert not np.isnan(each).any(),\
                f&#39;Driver dataset &#34;{name}&#34; contains NaNs&#39;
        tower_gpp = hdf[&#39;FLUXNET/GPP&#39;][:][:,pft_mask]
        # Read the validation mask; mask out observations that are
        #   reserved for validation
        print(&#39;Masking out validation data...&#39;)
        mask = hdf[&#39;FLUXNET/validation_mask&#39;][pft]
        tower_gpp[mask] = np.nan

    # Clean observations, then mask out driver data where the are no
    #   observations
    tower_gpp = self.clean_observed(
        tower_gpp, drivers, MOD17StochasticSampler.required_drivers[&#39;GPP&#39;],
        protocol = &#39;GPP&#39;)
    if weights is not None:
        weights = weights[~np.isnan(tower_gpp)]
    for d, _ in enumerate(drivers):
        drivers[d] = drivers[d][~np.isnan(tower_gpp)]
    tower_gpp = tower_gpp[~np.isnan(tower_gpp)]

    print(&#39;Initializing sampler...&#39;)
    backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] % (&#39;GPP&#39;, pft)
    sampler = MOD17StochasticSampler(
        self.config, MOD17._gpp, params_dict, backend = backend,
        weights = weights)
    if plot_trace or ipdb:
        if ipdb:
            import ipdb
            ipdb.set_trace()
        trace = sampler.get_trace()
        az.plot_trace(trace, var_names = MOD17.required_parameters[0:5])
        pyplot.show()
        return
    # Get (informative) priors for just those parameters that have them
    with open(self.config[&#39;optimization&#39;][&#39;prior&#39;], &#39;r&#39;) as file:
        prior = json.load(file)
    prior_params = filter(
        lambda p: p in prior.keys(), sampler.required_parameters[&#39;GPP&#39;])
    prior = dict([
        (p, {&#39;mu&#39;: prior[p][&#39;mu&#39;][pft], &#39;sigma&#39;: prior[p][&#39;sigma&#39;][pft]})
        for p in prior_params
    ])
    sampler.run(
        tower_gpp, drivers, prior = prior, save_fig = save_fig, **kwargs)</code></pre>
</details>
</dd>
<dt id="mod17.calibration.CalibrationAPI.tune_npp"><code class="name flex">
<span>def <span class="ident">tune_npp</span></span>(<span>self, pft: int, plot_trace: bool = False, ipdb: bool = False, save_fig: bool = False, climatology=False, cutoff: numbers.Number = 2385, k_folds: int = 1, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Run the MOD17 NPP calibration. If k-folds cross-validation is used,
the model is calibrated on $k$ random subsets of the data and a
series of file is created, e.g., as:</p>
<pre><code>MOD17_NPP_calibration_PFT1.h5
MOD17_NPP-k1_calibration_PFT1.nc4
MOD17_NPP-k2_calibration_PFT1.nc4
...
</code></pre>
<p>Where each <code>.nc4</code> file is a standard <code>arviz</code> backend and the <code>.h5</code>
indicates which indices from the NPP observations vector, after
removing NaNs, were excluded (i.e., the indices of the test data).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>pft</code></strong> :&ensp;<code>int</code></dt>
<dd>The Plant Functional Type (PFT) to calibrate</dd>
<dt><strong><code>plot_trace</code></strong> :&ensp;<code>bool</code></dt>
<dd>True to display the trace plot ONLY and not run calibration
(Default: False)</dd>
<dt><strong><code>ipdb</code></strong> :&ensp;<code>bool</code></dt>
<dd>True to drop into an interactive Python debugger (<code>ipdb</code>) after
loading an existing trace (Default: False)</dd>
<dt><strong><code>save_fig</code></strong> :&ensp;<code>bool</code></dt>
<dd>True to save the post-calibration trace plot to a file instead of
displaying it (Default: False)</dd>
<dt><strong><code>climatology</code></strong> :&ensp;<code>bool</code></dt>
<dd>True to use a MERRA-2 climatology (and look for it in the drivers
file), i.e., use <code>MERRA2_climatology</code> group instead of
<code>surface_met_MERRA2</code> group (Default: False)</dd>
<dt><strong><code>cutoff</code></strong> :&ensp;<code>Number</code></dt>
<dd>Maximum value of observed NPP (g C m-2 year-1); values above this
cutoff will be discarded and not used in calibration
(Default: 2385)</dd>
<dt><strong><code>k_folds</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of folds to use in k-folds cross-validation; defaults to
k=1, i.e., no cross-validation is performed.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments passed to
<code><a title="mod17.calibration.MOD17StochasticSampler.run" href="#mod17.calibration.StochasticSampler.run">StochasticSampler.run()</a></code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tune_npp(
        self, pft: int, plot_trace: bool = False, ipdb: bool = False,
        save_fig: bool = False, climatology = False,
        cutoff: Number = 2385, k_folds: int = 1, **kwargs):
    r&#39;&#39;&#39;
    Run the MOD17 NPP calibration. If k-folds cross-validation is used,
    the model is calibrated on $k$ random subsets of the data and a
    series of file is created, e.g., as:

        MOD17_NPP_calibration_PFT1.h5
        MOD17_NPP-k1_calibration_PFT1.nc4
        MOD17_NPP-k2_calibration_PFT1.nc4
        ...

    Where each `.nc4` file is a standard `arviz` backend and the `.h5`
    indicates which indices from the NPP observations vector, after
    removing NaNs, were excluded (i.e., the indices of the test data).

    Parameters
    ----------
    pft : int
        The Plant Functional Type (PFT) to calibrate
    plot_trace : bool
        True to display the trace plot ONLY and not run calibration
        (Default: False)
    ipdb : bool
        True to drop into an interactive Python debugger (`ipdb`) after
        loading an existing trace (Default: False)
    save_fig : bool
        True to save the post-calibration trace plot to a file instead of
        displaying it (Default: False)
    climatology : bool
        True to use a MERRA-2 climatology (and look for it in the drivers
        file), i.e., use `MERRA2_climatology` group instead of
        `surface_met_MERRA2` group (Default: False)
    cutoff : Number
        Maximum value of observed NPP (g C m-2 year-1); values above this
        cutoff will be discarded and not used in calibration
        (Default: 2385)
    k_folds : int
        Number of folds to use in k-folds cross-validation; defaults to
        k=1, i.e., no cross-validation is performed.
    **kwargs
        Additional keyword arguments passed to
        `MOD17StochasticSampler.run()`
    &#39;&#39;&#39;
    assert pft in PFT_VALID, f&#39;Invalid PFT: {pft}&#39;
    prefix = &#39;MERRA2_climatology&#39; if climatology else &#39;surface_met_MERRA2&#39;
    params_dict = restore_bplut(self.config[&#39;BPLUT&#39;][&#39;NPP&#39;])
    # Filter the parameters to just those for the PFT of interest
    params_dict = dict([(k, v[pft]) for k, v in params_dict.items()])
    model = MOD17(params_dict)
    kwargs.update({&#39;var_names&#39;: [
        &#39;~LUE_max&#39;, &#39;~tmin0&#39;, &#39;~tmin1&#39;, &#39;~vpd0&#39;, &#39;~vpd1&#39;, &#39;~log_likelihood&#39;
    ]})
    # Pass configuration parameters to MOD17StochasticSampler.run()
    for key in (&#39;chains&#39;, &#39;draws&#39;, &#39;tune&#39;, &#39;scaling&#39;):
        if key in self.config[&#39;optimization&#39;].keys():
            kwargs[key] = self.config[&#39;optimization&#39;][key]
    print(&#39;Loading driver datasets...&#39;)
    with h5py.File(self.hdf5, &#39;r&#39;) as hdf:
        # NOTE: This is only recorded at the site-level; no need to
        #   determine modal PFT across subgrid
        pft_map = hdf[&#39;NPP/PFT&#39;][:]
        # Leave out sites where there is no fPAR (and no LAI) data
        fpar = hdf[&#39;NPP/MOD15A2H_fPAR_clim_filt&#39;][:]
        mask = np.logical_and(
                pft_map == pft, ~np.isnan(np.nanmean(fpar, axis = -1))\
            .all(axis = 0))
        # NOTE: Converting from Kelvin to Celsius
        tday = hdf[f&#39;NPP/{prefix}/T10M_daytime&#39;][:][:,mask] - 273.15
        qv10m = hdf[f&#39;NPP/{prefix}/QV10M_daytime&#39;][:][:,mask]
        ps = hdf[f&#39;NPP/{prefix}/PS_daytime&#39;][:][:,mask]
        drivers = [ # fPAR, Tmin, VPD, PAR, LAI, Tmean, years
            hdf[&#39;NPP/MOD15A2H_fPAR_clim_filt&#39;][:][:,mask],
            hdf[f&#39;NPP/{prefix}/Tmin&#39;][:][:,mask]  - 273.15,
            MOD17.vpd(qv10m, ps, tday),
            MOD17.par(hdf[f&#39;NPP/{prefix}/SWGDN&#39;][:][:,mask]),
            hdf[&#39;NPP/MOD15A2H_LAI_clim_filt&#39;][:][:,mask],
            hdf[f&#39;NPP/{prefix}/T10M&#39;][:][:,mask] - 273.15,
            np.full(ps.shape, 1) # i.e., A 365-day climatological year (&#34;Year 1&#34;)
        ]
        observed_npp = hdf[&#39;NPP/NPP_total&#39;][:][mask]
    if cutoff is not None:
        observed_npp[observed_npp &gt; cutoff] = np.nan
    # Set negative VPD to zero
    drivers[2] = np.where(drivers[2] &lt; 0, 0, drivers[2])
    # Convert fPAR from (%) to [0,1] and re-scale LAI; reshape fPAR, LAI
    drivers[0] = np.nanmean(drivers[0], axis = -1) * 0.01
    drivers[4] = np.nanmean(drivers[4], axis = -1) * 0.1
    # TODO Mask out driver data where the are no observations
    for d, _ in enumerate(drivers):
        drivers[d] = drivers[d][:,~np.isnan(observed_npp)]
    observed_npp = observed_npp[~np.isnan(observed_npp)]

    if k_folds &gt; 1:
        # Back-up the original (complete) datasets
        _drivers = [d.copy() for d in drivers]
        _observed_npp = observed_npp.copy()
        # Randomize the indices of the NPP data
        indices = np.arange(0, observed_npp.size)
        np.random.shuffle(indices)
        # Get the starting and ending index of each fold
        fold_idx = np.array([indices.size // k_folds] * k_folds) * np.arange(0, k_folds)
        fold_idx = list(map(list, zip(fold_idx, fold_idx + indices.size // k_folds)))
        # Ensure that the entire dataset is used
        fold_idx[-1][-1] = indices.max()
        idx_test = [indices[start:end] for start, end in fold_idx]

    # Loop over each fold (or the entire dataset, if num. folds == 1)
    for k, fold in enumerate(range(1, k_folds + 1)):
        backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] % (&#39;NPP&#39;, pft)
        if k_folds &gt; 1 and fold == 1:
            # Create an HDF5 file with the same name as the (original)
            #   netCDF4 back-end, store the test indices
            with h5py.File(backend.replace(&#39;nc4&#39;, &#39;h5&#39;), &#39;w&#39;) as hdf:
                out = list(idx_test)
                size = indices.size // k_folds
                try:
                    out = np.stack(out)
                except ValueError:
                    size = max((o.size for o in out))
                    for i in range(0, len(out)):
                        out[i] = np.concatenate((out[i], [np.nan] * (size - out[i].size)))
                hdf.create_dataset(
                    &#39;test_indices&#39;, (k_folds, size), np.int32, np.stack(out))
            # Restore the original NPP dataset
            observed_npp = _observed_npp.copy()
            # Set to NaN all the test indices
            idx = idx_test[k]
            observed_npp[idx] = np.nan
            # Same for drivers, after restoring from the original
            drivers = [d.copy()[:,~np.isnan(observed_npp)] for d in _drivers]
            observed_npp = observed_npp[~np.isnan(observed_npp)]
        # Use a different naming scheme for the backend
        if k_folds &gt; 1:
            backend = self.config[&#39;optimization&#39;][&#39;backend_template&#39;] % (f&#39;NPP-k{fold}&#39;, pft)

        print(&#39;Initializing sampler...&#39;)
        sampler = MOD17StochasticSampler(
            self.config, MOD17._npp, params_dict, backend = backend,
            model_name = &#39;NPP&#39;)
        if plot_trace or ipdb:
            if ipdb:
                import ipdb
                ipdb.set_trace()
            trace = sampler.get_trace()
            az.plot_trace(
                trace, var_names = MOD17.required_parameters[5:])
            pyplot.show()
            return
        # Get (informative) priors for just those parameters that have them
        with open(self.config[&#39;optimization&#39;][&#39;prior&#39;], &#39;r&#39;) as file:
            prior = json.load(file)
        prior_params = filter(
            lambda p: p in prior.keys(), sampler.required_parameters[&#39;NPP&#39;])
        prior = dict([
            (p, prior[p]) for p in prior_params
        ])
        for key in prior.keys():
            # And make sure to subset to the chosen PFT!
            for arg in prior[key].keys():
                prior[key][arg] = prior[key][arg][pft]
        sampler.run(
            observed_npp, drivers, prior = prior, save_fig = save_fig,
            **kwargs)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="mod17.calibration.MOD17StochasticSampler"><code class="flex name class">
<span>class <span class="ident">MOD17StochasticSampler</span></span>
<span>(</span><span>config: dict, model: Callable, params_dict: dict = None, backend: str = None, weights: Sequence[+T_co] = None, model_name: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>A Markov Chain-Monte Carlo (MCMC) sampler for MOD17. The specific sampler
used is the Differential Evolution (DE) MCMC algorithm described by
Ter Braak (2008), though the implementation is specific to the PyMC3
library.</p>
<p>Considerations:</p>
<ol>
<li>Tower GPP is censored when values are &lt; 0 or when APAR is
&lt; 0.1 MJ m-2 d-1.</li>
</ol>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>config</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary of configuration parameters</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>Callable</code></dt>
<dd>The function to call (with driver data and parameters); this function
should take driver data as positional arguments and the model
parameters as a <code>*Sequence</code>; it should require no external state.</dd>
<dt><strong><code>observed</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>Sequence of observed values that will be used to calibrate the model;
i.e., model is scored by how close its predicted values are to the
observed values</dd>
<dt><strong><code>params_dict</code></strong> :&ensp;<code>dict</code> or <code>None</code></dt>
<dd>Dictionary of model parameters, to be used as initial values and as
the basis for constructing a new dictionary of optimized parameters</dd>
<dt><strong><code>backend</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>Path to a NetCDF4 file backend (Default: None)</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>Sequence</code> or <code>None</code></dt>
<dd>Optional sequence of weights applied to the model residuals (as in
weighted least squares)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MOD17StochasticSampler(StochasticSampler):
    &#39;&#39;&#39;
    A Markov Chain-Monte Carlo (MCMC) sampler for MOD17. The specific sampler
    used is the Differential Evolution (DE) MCMC algorithm described by
    Ter Braak (2008), though the implementation is specific to the PyMC3
    library.

    Considerations:

    1. Tower GPP is censored when values are &lt; 0 or when APAR is
        &lt; 0.1 MJ m-2 d-1.

    Parameters
    ----------
    config : dict
        Dictionary of configuration parameters
    model : Callable
        The function to call (with driver data and parameters); this function
        should take driver data as positional arguments and the model
        parameters as a `*Sequence`; it should require no external state.
    observed : Sequence
        Sequence of observed values that will be used to calibrate the model;
        i.e., model is scored by how close its predicted values are to the
        observed values
    params_dict : dict or None
        Dictionary of model parameters, to be used as initial values and as
        the basis for constructing a new dictionary of optimized parameters
    backend : str or None
        Path to a NetCDF4 file backend (Default: None)
    weights : Sequence or None
        Optional sequence of weights applied to the model residuals (as in
        weighted least squares)
    &#39;&#39;&#39;
    # NOTE: This is different than for mod17.MOD17 because we haven&#39;t yet
    #   figured out how the respiration terms are calculated
    required_parameters = {
        &#39;GPP&#39;: [&#39;LUE_max&#39;, &#39;tmin0&#39;, &#39;tmin1&#39;, &#39;vpd0&#39;, &#39;vpd1&#39;],
        &#39;NPP&#39;: MOD17.required_parameters
    }
    required_drivers = {
        &#39;GPP&#39;: [&#39;fPAR&#39;, &#39;Tmin&#39;, &#39;VPD&#39;, &#39;PAR&#39;],
        &#39;NPP&#39;: [&#39;fPAR&#39;, &#39;Tmin&#39;, &#39;VPD&#39;, &#39;PAR&#39;, &#39;LAI&#39;, &#39;Tmean&#39;, &#39;years&#39;]
    }

    def compile_gpp_model(
            self, observed: Sequence, drivers: Sequence) -&gt; pm.Model:
        &#39;&#39;&#39;
        Creates a new GPP model based on the prior distribution. Model can be
        re-compiled multiple times, e.g., for cross validation.

        Parameters
        ----------
        observed : Sequence
            Sequence of observed values that will be used to calibrate the model;
            i.e., model is scored by how close its predicted values are to the
            observed values
        drivers : list or tuple
            Sequence of driver datasets to be supplied, in order, to the
            model&#39;s run function

        Returns
        -------
        pm.Model
        &#39;&#39;&#39;
        # Define the objective/ likelihood function
        log_likelihood = BlackBoxLikelihood(
            self.model, observed, x = drivers, weights = self.weights,
            objective = self.config[&#39;optimization&#39;][&#39;objective&#39;].lower())
        # With this context manager, &#34;all PyMC3 objects introduced in the indented
        #   code block...are added to the model behind the scenes.&#34;
        with pm.Model() as model:
            # (Stochstic) Priors for unknown model parameters
            LUE_max = pm.TruncatedNormal(&#39;LUE_max&#39;,
                **self.prior[&#39;LUE_max&#39;], **self.bounds[&#39;LUE_max&#39;])
            # NOTE: tmin0, vpd0 are fixed at Collection 6.1 values
            tmin0 = self.params[&#39;tmin0&#39;]
            tmin1 = pm.Uniform(&#39;tmin1&#39;, **self.bounds[&#39;tmin1&#39;])
            # NOTE: Upper bound on `vpd1` is set by the maximum observed VPD
            vpd0 = self.params[&#39;vpd0&#39;]
            vpd1 = pm.Uniform(&#39;vpd1&#39;,
                lower = self.bounds[&#39;vpd1&#39;][&#39;lower&#39;],
                upper = drivers[2].max().round(0))
            # Convert model parameters to a tensor vector
            params_list = [LUE_max, tmin0, tmin1, vpd0, vpd1]
            params = at.as_tensor_variable(params_list)
            # Key step: Define the log-likelihood as an added potential
            pm.Potential(&#39;likelihood&#39;, log_likelihood(params))
        return model

    def compile_npp_model(
            self, observed: Sequence, drivers: Sequence) -&gt; pm.Model:
        &#39;&#39;&#39;
        Creates a new NPP model based on the prior distribution. Model can be
        re-compiled multiple times, e.g., for cross validation.

        Parameters
        ----------
        observed : Sequence
            Sequence of observed values that will be used to calibrate the model;
            i.e., model is scored by how close its predicted values are to the
            observed values
        drivers : list or tuple
            Sequence of driver datasets to be supplied, in order, to the
            model&#39;s run function

        Returns
        -------
        pm.Model
        &#39;&#39;&#39;
        # Define the objective/ likelihood function
        log_likelihood = BlackBoxLikelihood(
            self.model, observed, x = drivers, weights = self.weights,
            objective = self.config[&#39;optimization&#39;][&#39;objective&#39;].lower())
        # With this context manager, &#34;all PyMC3 objects introduced in the indented
        #   code block...are added to the model behind the scenes.&#34;
        with pm.Model() as model:
            # Setting GPP parameters that are known--EXCEPT tmin1
            LUE_max = self.params[&#39;LUE_max&#39;]
            tmin0   = self.params[&#39;tmin0&#39;]
            tmin1   = self.params[&#39;tmin1&#39;]
            vpd0    = self.params[&#39;vpd0&#39;]
            vpd1    = self.params[&#39;vpd1&#39;]
            # SLA fixed at prior mean
            SLA = np.exp(self.prior[&#39;SLA&#39;][&#39;mu&#39;])
            # Allometry ratios prescribe narrow range around Collection 6.1 values
            froot_leaf_ratio = pm.Triangular(
                &#39;froot_leaf_ratio&#39;, **self.prior[&#39;froot_leaf_ratio&#39;])
            # (Stochstic) Priors for unknown model parameters
            Q10_froot = pm.TruncatedNormal(
                &#39;Q10_froot&#39;, **self.prior[&#39;Q10_froot&#39;], **self.bounds[&#39;Q10&#39;])
            leaf_mr_base = pm.LogNormal(
                &#39;leaf_mr_base&#39;, **self.prior[&#39;leaf_mr_base&#39;])
            froot_mr_base = pm.LogNormal(
                &#39;froot_mr_base&#39;, **self.prior[&#39;froot_mr_base&#39;])
            # For GRS and CRO, livewood mass and respiration are zero
            if np.equal(list(self.prior[&#39;livewood_mr_base&#39;].values()), [0, 0]).all():
                livewood_leaf_ratio = 0
                livewood_mr_base = 0
                Q10_livewood = 0
            else:
                livewood_leaf_ratio = pm.Triangular(
                    &#39;livewood_leaf_ratio&#39;, **self.prior[&#39;livewood_leaf_ratio&#39;])
                livewood_mr_base = pm.LogNormal(
                    &#39;livewood_mr_base&#39;, **self.prior[&#39;livewood_mr_base&#39;])
                Q10_livewood = pm.TruncatedNormal(
                    &#39;Q10_livewood&#39;, **self.prior[&#39;Q10_livewood&#39;],
                    **self.bounds[&#39;Q10&#39;])
            # Convert model parameters to a tensor vector
            params_list = [
                LUE_max, tmin0, tmin1, vpd0, vpd1, SLA,
                Q10_livewood, Q10_froot, froot_leaf_ratio, livewood_leaf_ratio,
                leaf_mr_base, froot_mr_base, livewood_mr_base
            ]
            params = at.as_tensor_variable(params_list)
            # Key step: Define the log-likelihood as an added potential
            pm.Potential(&#39;likelihood&#39;, log_likelihood(params))
        return model</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mod17.calibration.StochasticSampler" href="#mod17.calibration.StochasticSampler">StochasticSampler</a></li>
<li><a title="mod17.calibration.AbstractSampler" href="#mod17.calibration.AbstractSampler">AbstractSampler</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mod17.viirs.VNP17StochasticSampler" href="viirs.html#mod17.viirs.VNP17StochasticSampler">VNP17StochasticSampler</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="mod17.calibration.MOD17StochasticSampler.required_drivers"><code class="name">var <span class="ident">required_drivers</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mod17.calibration.MOD17StochasticSampler.required_parameters"><code class="name">var <span class="ident">required_parameters</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mod17.calibration.MOD17StochasticSampler.compile_gpp_model"><code class="name flex">
<span>def <span class="ident">compile_gpp_model</span></span>(<span>self, observed: Sequence[+T_co], drivers: Sequence[+T_co]) ‑> pymc.model.Model</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a new GPP model based on the prior distribution. Model can be
re-compiled multiple times, e.g., for cross validation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>observed</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>Sequence of observed values that will be used to calibrate the model;
i.e., model is scored by how close its predicted values are to the
observed values</dd>
<dt><strong><code>drivers</code></strong> :&ensp;<code>list</code> or <code>tuple</code></dt>
<dd>Sequence of driver datasets to be supplied, in order, to the
model's run function</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pm.Model</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_gpp_model(
        self, observed: Sequence, drivers: Sequence) -&gt; pm.Model:
    &#39;&#39;&#39;
    Creates a new GPP model based on the prior distribution. Model can be
    re-compiled multiple times, e.g., for cross validation.

    Parameters
    ----------
    observed : Sequence
        Sequence of observed values that will be used to calibrate the model;
        i.e., model is scored by how close its predicted values are to the
        observed values
    drivers : list or tuple
        Sequence of driver datasets to be supplied, in order, to the
        model&#39;s run function

    Returns
    -------
    pm.Model
    &#39;&#39;&#39;
    # Define the objective/ likelihood function
    log_likelihood = BlackBoxLikelihood(
        self.model, observed, x = drivers, weights = self.weights,
        objective = self.config[&#39;optimization&#39;][&#39;objective&#39;].lower())
    # With this context manager, &#34;all PyMC3 objects introduced in the indented
    #   code block...are added to the model behind the scenes.&#34;
    with pm.Model() as model:
        # (Stochstic) Priors for unknown model parameters
        LUE_max = pm.TruncatedNormal(&#39;LUE_max&#39;,
            **self.prior[&#39;LUE_max&#39;], **self.bounds[&#39;LUE_max&#39;])
        # NOTE: tmin0, vpd0 are fixed at Collection 6.1 values
        tmin0 = self.params[&#39;tmin0&#39;]
        tmin1 = pm.Uniform(&#39;tmin1&#39;, **self.bounds[&#39;tmin1&#39;])
        # NOTE: Upper bound on `vpd1` is set by the maximum observed VPD
        vpd0 = self.params[&#39;vpd0&#39;]
        vpd1 = pm.Uniform(&#39;vpd1&#39;,
            lower = self.bounds[&#39;vpd1&#39;][&#39;lower&#39;],
            upper = drivers[2].max().round(0))
        # Convert model parameters to a tensor vector
        params_list = [LUE_max, tmin0, tmin1, vpd0, vpd1]
        params = at.as_tensor_variable(params_list)
        # Key step: Define the log-likelihood as an added potential
        pm.Potential(&#39;likelihood&#39;, log_likelihood(params))
    return model</code></pre>
</details>
</dd>
<dt id="mod17.calibration.MOD17StochasticSampler.compile_npp_model"><code class="name flex">
<span>def <span class="ident">compile_npp_model</span></span>(<span>self, observed: Sequence[+T_co], drivers: Sequence[+T_co]) ‑> pymc.model.Model</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a new NPP model based on the prior distribution. Model can be
re-compiled multiple times, e.g., for cross validation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>observed</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>Sequence of observed values that will be used to calibrate the model;
i.e., model is scored by how close its predicted values are to the
observed values</dd>
<dt><strong><code>drivers</code></strong> :&ensp;<code>list</code> or <code>tuple</code></dt>
<dd>Sequence of driver datasets to be supplied, in order, to the
model's run function</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pm.Model</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_npp_model(
        self, observed: Sequence, drivers: Sequence) -&gt; pm.Model:
    &#39;&#39;&#39;
    Creates a new NPP model based on the prior distribution. Model can be
    re-compiled multiple times, e.g., for cross validation.

    Parameters
    ----------
    observed : Sequence
        Sequence of observed values that will be used to calibrate the model;
        i.e., model is scored by how close its predicted values are to the
        observed values
    drivers : list or tuple
        Sequence of driver datasets to be supplied, in order, to the
        model&#39;s run function

    Returns
    -------
    pm.Model
    &#39;&#39;&#39;
    # Define the objective/ likelihood function
    log_likelihood = BlackBoxLikelihood(
        self.model, observed, x = drivers, weights = self.weights,
        objective = self.config[&#39;optimization&#39;][&#39;objective&#39;].lower())
    # With this context manager, &#34;all PyMC3 objects introduced in the indented
    #   code block...are added to the model behind the scenes.&#34;
    with pm.Model() as model:
        # Setting GPP parameters that are known--EXCEPT tmin1
        LUE_max = self.params[&#39;LUE_max&#39;]
        tmin0   = self.params[&#39;tmin0&#39;]
        tmin1   = self.params[&#39;tmin1&#39;]
        vpd0    = self.params[&#39;vpd0&#39;]
        vpd1    = self.params[&#39;vpd1&#39;]
        # SLA fixed at prior mean
        SLA = np.exp(self.prior[&#39;SLA&#39;][&#39;mu&#39;])
        # Allometry ratios prescribe narrow range around Collection 6.1 values
        froot_leaf_ratio = pm.Triangular(
            &#39;froot_leaf_ratio&#39;, **self.prior[&#39;froot_leaf_ratio&#39;])
        # (Stochstic) Priors for unknown model parameters
        Q10_froot = pm.TruncatedNormal(
            &#39;Q10_froot&#39;, **self.prior[&#39;Q10_froot&#39;], **self.bounds[&#39;Q10&#39;])
        leaf_mr_base = pm.LogNormal(
            &#39;leaf_mr_base&#39;, **self.prior[&#39;leaf_mr_base&#39;])
        froot_mr_base = pm.LogNormal(
            &#39;froot_mr_base&#39;, **self.prior[&#39;froot_mr_base&#39;])
        # For GRS and CRO, livewood mass and respiration are zero
        if np.equal(list(self.prior[&#39;livewood_mr_base&#39;].values()), [0, 0]).all():
            livewood_leaf_ratio = 0
            livewood_mr_base = 0
            Q10_livewood = 0
        else:
            livewood_leaf_ratio = pm.Triangular(
                &#39;livewood_leaf_ratio&#39;, **self.prior[&#39;livewood_leaf_ratio&#39;])
            livewood_mr_base = pm.LogNormal(
                &#39;livewood_mr_base&#39;, **self.prior[&#39;livewood_mr_base&#39;])
            Q10_livewood = pm.TruncatedNormal(
                &#39;Q10_livewood&#39;, **self.prior[&#39;Q10_livewood&#39;],
                **self.bounds[&#39;Q10&#39;])
        # Convert model parameters to a tensor vector
        params_list = [
            LUE_max, tmin0, tmin1, vpd0, vpd1, SLA,
            Q10_livewood, Q10_froot, froot_leaf_ratio, livewood_leaf_ratio,
            leaf_mr_base, froot_mr_base, livewood_mr_base
        ]
        params = at.as_tensor_variable(params_list)
        # Key step: Define the log-likelihood as an added potential
        pm.Potential(&#39;likelihood&#39;, log_likelihood(params))
    return model</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mod17.calibration.StochasticSampler" href="#mod17.calibration.StochasticSampler">StochasticSampler</a></b></code>:
<ul class="hlist">
<li><code><a title="mod17.calibration.StochasticSampler.get_posterior" href="#mod17.calibration.AbstractSampler.get_posterior">get_posterior</a></code></li>
<li><code><a title="mod17.calibration.StochasticSampler.get_trace" href="#mod17.calibration.AbstractSampler.get_trace">get_trace</a></code></li>
<li><code><a title="mod17.calibration.StochasticSampler.plot_autocorr" href="#mod17.calibration.AbstractSampler.plot_autocorr">plot_autocorr</a></code></li>
<li><code><a title="mod17.calibration.StochasticSampler.plot_forest" href="#mod17.calibration.AbstractSampler.plot_forest">plot_forest</a></code></li>
<li><code><a title="mod17.calibration.StochasticSampler.plot_pair" href="#mod17.calibration.AbstractSampler.plot_pair">plot_pair</a></code></li>
<li><code><a title="mod17.calibration.StochasticSampler.plot_partial_score" href="#mod17.calibration.AbstractSampler.plot_partial_score">plot_partial_score</a></code></li>
<li><code><a title="mod17.calibration.StochasticSampler.plot_posterior" href="#mod17.calibration.AbstractSampler.plot_posterior">plot_posterior</a></code></li>
<li><code><a title="mod17.calibration.StochasticSampler.run" href="#mod17.calibration.StochasticSampler.run">run</a></code></li>
<li><code><a title="mod17.calibration.StochasticSampler.score_posterior" href="#mod17.calibration.AbstractSampler.score_posterior">score_posterior</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mod17.calibration.StochasticSampler"><code class="flex name class">
<span>class <span class="ident">StochasticSampler</span></span>
<span>(</span><span>config: dict, model: Callable, params_dict: dict = None, backend: str = None, weights: Sequence[+T_co] = None, model_name: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>A Markov Chain-Monte Carlo (MCMC) sampler for an arbitrary model. The
specific sampler used is the Differential Evolution (DE) MCMC algorithm
described by Ter Braak (2008), though the implementation is specific to
the PyMC3 library.</p>
<p>NOTE: The <code>model</code> (a function) should be named "_name" where "name" is
some uniquely identifiable model name. This helps <code><a title="mod17.calibration.StochasticSampler.run" href="#mod17.calibration.StochasticSampler.run">StochasticSampler.run()</a></code>
to find the correct compiler for the model. The compiler function should
be named <code>compiled_name_model()</code> (where "name" is the model name) and be
defined on a subclass of <code><a title="mod17.calibration.StochasticSampler" href="#mod17.calibration.StochasticSampler">StochasticSampler</a></code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>config</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary of configuration parameters</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>Callable</code></dt>
<dd>The function to call (with driver data and parameters); this function
should take driver data as positional arguments and the model
parameters as a <code>*Sequence</code>; it should require no external state.</dd>
<dt><strong><code>observed</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>Sequence of observed values that will be used to calibrate the model;
i.e., model is scored by how close its predicted values are to the
observed values</dd>
<dt><strong><code>params_dict</code></strong> :&ensp;<code>dict</code> or <code>None</code></dt>
<dd>Dictionary of model parameters, to be used as initial values and as
the basis for constructing a new dictionary of optimized parameters</dd>
<dt><strong><code>backend</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>Path to a NetCDF4 file backend (Default: None)</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>Sequence</code> or <code>None</code></dt>
<dd>Optional sequence of weights applied to the model residuals (as in
weighted least squares)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StochasticSampler(AbstractSampler):
    &#39;&#39;&#39;
    A Markov Chain-Monte Carlo (MCMC) sampler for an arbitrary model. The
    specific sampler used is the Differential Evolution (DE) MCMC algorithm
    described by Ter Braak (2008), though the implementation is specific to
    the PyMC3 library.

    NOTE: The `model` (a function) should be named &#34;_name&#34; where &#34;name&#34; is
    some uniquely identifiable model name. This helps `StochasticSampler.run()`
    to find the correct compiler for the model. The compiler function should
    be named `compiled_name_model()` (where &#34;name&#34; is the model name) and be
    defined on a subclass of `StochasticSampler`.

    Parameters
    ----------
    config : dict
        Dictionary of configuration parameters
    model : Callable
        The function to call (with driver data and parameters); this function
        should take driver data as positional arguments and the model
        parameters as a `*Sequence`; it should require no external state.
    observed : Sequence
        Sequence of observed values that will be used to calibrate the model;
        i.e., model is scored by how close its predicted values are to the
        observed values
    params_dict : dict or None
        Dictionary of model parameters, to be used as initial values and as
        the basis for constructing a new dictionary of optimized parameters
    backend : str or None
        Path to a NetCDF4 file backend (Default: None)
    weights : Sequence or None
        Optional sequence of weights applied to the model residuals (as in
        weighted least squares)
    &#39;&#39;&#39;
    def __init__(
            self, config: dict, model: Callable, params_dict: dict = None,
            backend: str = None, weights: Sequence = None,
            model_name: str = None):
        self.backend = backend
        # Convert the BOUNDS into nested dicts for easy use
        self.bounds = dict([
            (key, dict([(&#39;lower&#39;, b[0]), (&#39;upper&#39;, b[1])]))
            for key, b in config[&#39;optimization&#39;][&#39;bounds&#39;].items()
        ])
        self.config = config
        self.model = model
        if hasattr(model, &#39;__name__&#39;):
            self.name = model.__name__.strip(&#39;_&#39;).upper() # &#34;_gpp&#34; = &#34;GPP&#34;
        else:
            self.name = model_name
        self.params = params_dict
        # Set the model&#39;s prior distribution assumptions
        self.prior = dict()
        for key in self.required_parameters[self.name]:
            # NOTE: This is the default only for LUE_max; other parameters,
            #   with Uniform distributions, don&#39;t use anything here
            self.prior.setdefault(key, {
                &#39;mu&#39;: params_dict[key],
                &#39;sigma&#39;: 2e-4
            })
        self.weights = weights
        assert os.path.exists(os.path.dirname(backend))

    def run(
            self, observed: Sequence, drivers: Sequence,
            draws = 1000, chains = 3, tune = &#39;lambda&#39;,
            scaling: float = 1e-3, prior: dict = dict(),
            check_shape: bool = False, save_fig: bool = False,
            var_names: Sequence = None) -&gt; None:
        &#39;&#39;&#39;
        Fits the model using DE-MCMCz approach. `tune=&#34;lambda&#34;` (default) is
        recommended; lambda is related to the scale of the jumps learned from
        other chains, but epsilon (&#34;scaling&#34;) controls the scale directly.
        Using a larger value for `scaling` (Default: 1e-3) will produce larger
        jumps and may directly address &#34;sticky&#34; chains.

        Parameters
        ----------
        observed : Sequence
            The observed data the model will be calibrated against
        drivers : list or tuple
            Sequence of driver datasets to be supplied, in order, to the
            model&#39;s run function
        draws : int
            Number of samples to draw (on each chain); defaults to 1000
        chains : int
            Number of chains; defaults to 3
        tune : str or None
            Which hyperparameter to tune: Defaults to &#39;lambda&#39;, but can also
            be &#39;scaling&#39; or None.
        scaling : float
            Initial scale factor for epsilon (Default: 1e-3)
        prior : dict
        check_shape : bool
            True to require that input driver datasets have the same shape as
            the observed values (Default: False)
        save_fig : bool
            True to save figures to files instead of showing them
            (Default: False)
        var_names : Sequence
            One or more variable names to show in the plot
        &#39;&#39;&#39;
        assert not check_shape or drivers[0].shape == observed.shape,\
            &#39;Driver data should have the same shape as the &#34;observed&#34; data&#39;
        assert len(drivers) == len(self.required_drivers[self.name]),\
            &#39;Did not receive expected number of driver datasets!&#39;
        assert tune in (&#39;lambda&#39;, &#39;scaling&#39;) or tune is None
        # Update prior assumptions
        self.prior.update(prior)
        # Generate an initial goodness-of-fit score
        predicted = self.model([
            self.params[p] for p in self.required_parameters[self.name]
        ], *drivers)
        if self.weights is not None:
            score = np.sqrt(
                np.nanmean(((predicted - observed) * self.weights) ** 2))
        else:
            score = np.sqrt(np.nanmean(((predicted - observed)) ** 2))
        print(&#39;-- RMSD at the initial point: %.3f&#39; % score)
        print(&#39;Compiling model...&#39;)
        try:
            compiler = getattr(self, &#39;compile_%s_model&#39; % self.name.lower())
        except AttributeError:
            raise AttributeError(&#39;&#39;&#39;Could not find a compiler for model named
            &#34;%s&#34;; make sure that a function &#34;compile_%s_model()&#34; is defined on
             this class&#39;&#39;&#39; % (model_name, model_name))
        with compiler(observed, drivers) as model:
            with warnings.catch_warnings():
                warnings.simplefilter(&#39;ignore&#39;)
                step_func = pm.DEMetropolisZ(tune = tune, scaling = scaling)
                trace = pm.sample(
                    draws = draws, step = step_func, cores = chains,
                    chains = chains, idata_kwargs = {&#39;log_likelihood&#39;: True})
            if self.backend is not None:
                print(&#39;Writing results to file...&#39;)
                trace.to_netcdf(self.backend)
            if var_names is None:
                az.plot_trace(trace, var_names = [&#39;~log_likelihood&#39;])
            else:
                az.plot_trace(trace, var_names = var_names)
            if save_fig:
                pyplot.savefig(&#39;.&#39;.join(self.backend.split(&#39;.&#39;)[:-1]) + &#39;.png&#39;)
            else:
                pyplot.show()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mod17.calibration.AbstractSampler" href="#mod17.calibration.AbstractSampler">AbstractSampler</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mod17.calibration.MOD17StochasticSampler" href="#mod17.calibration.MOD17StochasticSampler">MOD17StochasticSampler</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mod17.calibration.StochasticSampler.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, observed: Sequence[+T_co], drivers: Sequence[+T_co], draws=1000, chains=3, tune='lambda', scaling: float = 0.001, prior: dict = {}, check_shape: bool = False, save_fig: bool = False, var_names: Sequence[+T_co] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Fits the model using DE-MCMCz approach. <code>tune="lambda"</code> (default) is
recommended; lambda is related to the scale of the jumps learned from
other chains, but epsilon ("scaling") controls the scale directly.
Using a larger value for <code>scaling</code> (Default: 1e-3) will produce larger
jumps and may directly address "sticky" chains.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>observed</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>The observed data the model will be calibrated against</dd>
<dt><strong><code>drivers</code></strong> :&ensp;<code>list</code> or <code>tuple</code></dt>
<dd>Sequence of driver datasets to be supplied, in order, to the
model's run function</dd>
<dt><strong><code>draws</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of samples to draw (on each chain); defaults to 1000</dd>
<dt><strong><code>chains</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of chains; defaults to 3</dd>
<dt><strong><code>tune</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>Which hyperparameter to tune: Defaults to 'lambda', but can also
be 'scaling' or None.</dd>
<dt><strong><code>scaling</code></strong> :&ensp;<code>float</code></dt>
<dd>Initial scale factor for epsilon (Default: 1e-3)</dd>
<dt><strong><code>prior</code></strong> :&ensp;<code>dict</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>check_shape</code></strong> :&ensp;<code>bool</code></dt>
<dd>True to require that input driver datasets have the same shape as
the observed values (Default: False)</dd>
<dt><strong><code>save_fig</code></strong> :&ensp;<code>bool</code></dt>
<dd>True to save figures to files instead of showing them
(Default: False)</dd>
<dt><strong><code>var_names</code></strong> :&ensp;<code>Sequence</code></dt>
<dd>One or more variable names to show in the plot</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(
        self, observed: Sequence, drivers: Sequence,
        draws = 1000, chains = 3, tune = &#39;lambda&#39;,
        scaling: float = 1e-3, prior: dict = dict(),
        check_shape: bool = False, save_fig: bool = False,
        var_names: Sequence = None) -&gt; None:
    &#39;&#39;&#39;
    Fits the model using DE-MCMCz approach. `tune=&#34;lambda&#34;` (default) is
    recommended; lambda is related to the scale of the jumps learned from
    other chains, but epsilon (&#34;scaling&#34;) controls the scale directly.
    Using a larger value for `scaling` (Default: 1e-3) will produce larger
    jumps and may directly address &#34;sticky&#34; chains.

    Parameters
    ----------
    observed : Sequence
        The observed data the model will be calibrated against
    drivers : list or tuple
        Sequence of driver datasets to be supplied, in order, to the
        model&#39;s run function
    draws : int
        Number of samples to draw (on each chain); defaults to 1000
    chains : int
        Number of chains; defaults to 3
    tune : str or None
        Which hyperparameter to tune: Defaults to &#39;lambda&#39;, but can also
        be &#39;scaling&#39; or None.
    scaling : float
        Initial scale factor for epsilon (Default: 1e-3)
    prior : dict
    check_shape : bool
        True to require that input driver datasets have the same shape as
        the observed values (Default: False)
    save_fig : bool
        True to save figures to files instead of showing them
        (Default: False)
    var_names : Sequence
        One or more variable names to show in the plot
    &#39;&#39;&#39;
    assert not check_shape or drivers[0].shape == observed.shape,\
        &#39;Driver data should have the same shape as the &#34;observed&#34; data&#39;
    assert len(drivers) == len(self.required_drivers[self.name]),\
        &#39;Did not receive expected number of driver datasets!&#39;
    assert tune in (&#39;lambda&#39;, &#39;scaling&#39;) or tune is None
    # Update prior assumptions
    self.prior.update(prior)
    # Generate an initial goodness-of-fit score
    predicted = self.model([
        self.params[p] for p in self.required_parameters[self.name]
    ], *drivers)
    if self.weights is not None:
        score = np.sqrt(
            np.nanmean(((predicted - observed) * self.weights) ** 2))
    else:
        score = np.sqrt(np.nanmean(((predicted - observed)) ** 2))
    print(&#39;-- RMSD at the initial point: %.3f&#39; % score)
    print(&#39;Compiling model...&#39;)
    try:
        compiler = getattr(self, &#39;compile_%s_model&#39; % self.name.lower())
    except AttributeError:
        raise AttributeError(&#39;&#39;&#39;Could not find a compiler for model named
        &#34;%s&#34;; make sure that a function &#34;compile_%s_model()&#34; is defined on
         this class&#39;&#39;&#39; % (model_name, model_name))
    with compiler(observed, drivers) as model:
        with warnings.catch_warnings():
            warnings.simplefilter(&#39;ignore&#39;)
            step_func = pm.DEMetropolisZ(tune = tune, scaling = scaling)
            trace = pm.sample(
                draws = draws, step = step_func, cores = chains,
                chains = chains, idata_kwargs = {&#39;log_likelihood&#39;: True})
        if self.backend is not None:
            print(&#39;Writing results to file...&#39;)
            trace.to_netcdf(self.backend)
        if var_names is None:
            az.plot_trace(trace, var_names = [&#39;~log_likelihood&#39;])
        else:
            az.plot_trace(trace, var_names = var_names)
        if save_fig:
            pyplot.savefig(&#39;.&#39;.join(self.backend.split(&#39;.&#39;)[:-1]) + &#39;.png&#39;)
        else:
            pyplot.show()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mod17.calibration.AbstractSampler" href="#mod17.calibration.AbstractSampler">AbstractSampler</a></b></code>:
<ul class="hlist">
<li><code><a title="mod17.calibration.AbstractSampler.get_posterior" href="#mod17.calibration.AbstractSampler.get_posterior">get_posterior</a></code></li>
<li><code><a title="mod17.calibration.AbstractSampler.get_trace" href="#mod17.calibration.AbstractSampler.get_trace">get_trace</a></code></li>
<li><code><a title="mod17.calibration.AbstractSampler.plot_autocorr" href="#mod17.calibration.AbstractSampler.plot_autocorr">plot_autocorr</a></code></li>
<li><code><a title="mod17.calibration.AbstractSampler.plot_forest" href="#mod17.calibration.AbstractSampler.plot_forest">plot_forest</a></code></li>
<li><code><a title="mod17.calibration.AbstractSampler.plot_pair" href="#mod17.calibration.AbstractSampler.plot_pair">plot_pair</a></code></li>
<li><code><a title="mod17.calibration.AbstractSampler.plot_partial_score" href="#mod17.calibration.AbstractSampler.plot_partial_score">plot_partial_score</a></code></li>
<li><code><a title="mod17.calibration.AbstractSampler.plot_posterior" href="#mod17.calibration.AbstractSampler.plot_posterior">plot_posterior</a></code></li>
<li><code><a title="mod17.calibration.AbstractSampler.score_posterior" href="#mod17.calibration.AbstractSampler.score_posterior">score_posterior</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="mod17" href="index.html">mod17</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mod17.calibration.AbstractSampler" href="#mod17.calibration.AbstractSampler">AbstractSampler</a></code></h4>
<ul class="two-column">
<li><code><a title="mod17.calibration.AbstractSampler.get_posterior" href="#mod17.calibration.AbstractSampler.get_posterior">get_posterior</a></code></li>
<li><code><a title="mod17.calibration.AbstractSampler.get_trace" href="#mod17.calibration.AbstractSampler.get_trace">get_trace</a></code></li>
<li><code><a title="mod17.calibration.AbstractSampler.plot_autocorr" href="#mod17.calibration.AbstractSampler.plot_autocorr">plot_autocorr</a></code></li>
<li><code><a title="mod17.calibration.AbstractSampler.plot_forest" href="#mod17.calibration.AbstractSampler.plot_forest">plot_forest</a></code></li>
<li><code><a title="mod17.calibration.AbstractSampler.plot_pair" href="#mod17.calibration.AbstractSampler.plot_pair">plot_pair</a></code></li>
<li><code><a title="mod17.calibration.AbstractSampler.plot_partial_score" href="#mod17.calibration.AbstractSampler.plot_partial_score">plot_partial_score</a></code></li>
<li><code><a title="mod17.calibration.AbstractSampler.plot_posterior" href="#mod17.calibration.AbstractSampler.plot_posterior">plot_posterior</a></code></li>
<li><code><a title="mod17.calibration.AbstractSampler.score_posterior" href="#mod17.calibration.AbstractSampler.score_posterior">score_posterior</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mod17.calibration.BlackBoxLikelihood" href="#mod17.calibration.BlackBoxLikelihood">BlackBoxLikelihood</a></code></h4>
<ul class="two-column">
<li><code><a title="mod17.calibration.BlackBoxLikelihood.default_output" href="#mod17.calibration.BlackBoxLikelihood.default_output">default_output</a></code></li>
<li><code><a title="mod17.calibration.BlackBoxLikelihood.destroy_map" href="#mod17.calibration.BlackBoxLikelihood.destroy_map">destroy_map</a></code></li>
<li><code><a title="mod17.calibration.BlackBoxLikelihood.itypes" href="#mod17.calibration.BlackBoxLikelihood.itypes">itypes</a></code></li>
<li><code><a title="mod17.calibration.BlackBoxLikelihood.loglik" href="#mod17.calibration.BlackBoxLikelihood.loglik">loglik</a></code></li>
<li><code><a title="mod17.calibration.BlackBoxLikelihood.loglik_gaussian" href="#mod17.calibration.BlackBoxLikelihood.loglik_gaussian">loglik_gaussian</a></code></li>
<li><code><a title="mod17.calibration.BlackBoxLikelihood.loglik_kge" href="#mod17.calibration.BlackBoxLikelihood.loglik_kge">loglik_kge</a></code></li>
<li><code><a title="mod17.calibration.BlackBoxLikelihood.otypes" href="#mod17.calibration.BlackBoxLikelihood.otypes">otypes</a></code></li>
<li><code><a title="mod17.calibration.BlackBoxLikelihood.params_type" href="#mod17.calibration.BlackBoxLikelihood.params_type">params_type</a></code></li>
<li><code><a title="mod17.calibration.BlackBoxLikelihood.perform" href="#mod17.calibration.BlackBoxLikelihood.perform">perform</a></code></li>
<li><code><a title="mod17.calibration.BlackBoxLikelihood.view_map" href="#mod17.calibration.BlackBoxLikelihood.view_map">view_map</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mod17.calibration.CalibrationAPI" href="#mod17.calibration.CalibrationAPI">CalibrationAPI</a></code></h4>
<ul class="">
<li><code><a title="mod17.calibration.CalibrationAPI.clean_observed" href="#mod17.calibration.CalibrationAPI.clean_observed">clean_observed</a></code></li>
<li><code><a title="mod17.calibration.CalibrationAPI.export_bplut" href="#mod17.calibration.CalibrationAPI.export_bplut">export_bplut</a></code></li>
<li><code><a title="mod17.calibration.CalibrationAPI.export_likely_posterior" href="#mod17.calibration.CalibrationAPI.export_likely_posterior">export_likely_posterior</a></code></li>
<li><code><a title="mod17.calibration.CalibrationAPI.export_posterior" href="#mod17.calibration.CalibrationAPI.export_posterior">export_posterior</a></code></li>
<li><code><a title="mod17.calibration.CalibrationAPI.tune_gpp" href="#mod17.calibration.CalibrationAPI.tune_gpp">tune_gpp</a></code></li>
<li><code><a title="mod17.calibration.CalibrationAPI.tune_npp" href="#mod17.calibration.CalibrationAPI.tune_npp">tune_npp</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mod17.calibration.MOD17StochasticSampler" href="#mod17.calibration.MOD17StochasticSampler">MOD17StochasticSampler</a></code></h4>
<ul class="">
<li><code><a title="mod17.calibration.MOD17StochasticSampler.compile_gpp_model" href="#mod17.calibration.MOD17StochasticSampler.compile_gpp_model">compile_gpp_model</a></code></li>
<li><code><a title="mod17.calibration.MOD17StochasticSampler.compile_npp_model" href="#mod17.calibration.MOD17StochasticSampler.compile_npp_model">compile_npp_model</a></code></li>
<li><code><a title="mod17.calibration.MOD17StochasticSampler.required_drivers" href="#mod17.calibration.MOD17StochasticSampler.required_drivers">required_drivers</a></code></li>
<li><code><a title="mod17.calibration.MOD17StochasticSampler.required_parameters" href="#mod17.calibration.MOD17StochasticSampler.required_parameters">required_parameters</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mod17.calibration.StochasticSampler" href="#mod17.calibration.StochasticSampler">StochasticSampler</a></code></h4>
<ul class="">
<li><code><a title="mod17.calibration.StochasticSampler.run" href="#mod17.calibration.StochasticSampler.run">run</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>